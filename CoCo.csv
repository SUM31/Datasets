_article_id,citation_contexts/citation_context/0/raw_citance,citation_contexts/citation_context/0/ctx_ref/0/_cit_str_original,citation_end,citation_contexts/citation_context/0/ctx_ref/0/_function,citation_contexts/citation_context/0/ctx_ref/0/_function_originally_annotated,citation_contexts/citation_context/0/ctx_ref/0/_polarity,citation_contexts_ref_id,citation_contexts/citation_context/0/ctx_ref/0/_reliable,citation_contexts_start,citation_contexts/citation_context/0/ctx_ref/0/_target,citation_contexts/citation_context/0/ctx_ref/0/_type,citation_contexts/citation_context/0/ctx_ref/0/__text,citation_contexts/citation_context/0/ctx_s/0/_ctx_id,citation_contexts/citation_context/0/ctx_s/0/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/0/_s_id,citation_contexts/citation_context/0/ctx_s/0/_useful,citation_contexts/citation_context/0/ctx_s/0/__text,citation_contexts/citation_context/0/ctx_s/1/_ctx_id,citation_contexts/citation_context/0/ctx_s/1/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/1/_s_id,citation_contexts/citation_context/0/ctx_s/1/_useful,citation_contexts/citation_context/0/ctx_s/1/__text,citation_contexts/citation_context/0/ctx_s/2/_ctx_id,citation_contexts/citation_context/0/ctx_s/2/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/2/_s_id,citation_contexts/citation_context/0/ctx_s/2/_useful,citation_contexts/citation_context/0/ctx_s/2/__text,citation_contexts/citation_context/0/ctx_s/3/_ctx_id,citation_contexts/citation_context/0/ctx_s/3/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/3/_s_id,citation_contexts/citation_context/0/ctx_s/3/_useful,citation_contexts/citation_context/0/ctx_s/3/__text,citation_contexts/citation_context/0/ctx_s/4/_ctx_id,citation_contexts/citation_context/0/ctx_s/4/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/4/_s_id,citation_contexts/citation_context/0/ctx_s/4/_useful,citation_contexts/citation_context/0/ctx_s/4/__text,citation_contexts/citation_context/0/ctx_s/5/_ctx_id,citation_contexts/citation_context/0/ctx_s/5/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/5/_s_id,citation_contexts/citation_context/0/ctx_s/5/_useful,citation_contexts/citation_context/0/ctx_s/5/__text,citation_contexts/citation_context/0/_mapped,citation_contexts/citation_context/0/_ref_ids,citation_contexts/citation_context/0/_reliable,citation_contexts/citation_context/0/_s_id,citation_contexts/citation_context/0/_section_header,citation_contexts/citation_context/0/_section_number,citation_contexts/citation_context/1/raw_citance,citation_contexts/citation_context/1/ctx_ref/0/_cit_str_original,citation_contexts/citation_context/1/ctx_ref/0/_end,citation_contexts/citation_context/1/ctx_ref/0/_function,citation_contexts/citation_context/1/ctx_ref/0/_function_originally_annotated,citation_contexts/citation_context/1/ctx_ref/0/_polarity,citation_contexts/citation_context/1/ctx_ref/0/_ref_id,citation_contexts/citation_context/1/ctx_ref/0/_reliable,citation_contexts/citation_context/1/ctx_ref/0/_start,citation_contexts/citation_context/1/ctx_ref/0/_target,citation_contexts/citation_context/1/ctx_ref/0/_type,citation_contexts/citation_context/1/ctx_ref/0/__text,citation_contexts/citation_context/1/ctx_s/0/_ctx_id,citation_contexts/citation_context/1/ctx_s/0/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/0/_s_id,citation_contexts/citation_context/1/ctx_s/0/_useful,citation_contexts/citation_context/1/ctx_s/0/__text,citation_contexts/citation_context/1/ctx_s/1/_ctx_id,citation_contexts/citation_context/1/ctx_s/1/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/1/_s_id,citation_contexts/citation_context/1/ctx_s/1/_useful,citation_contexts/citation_context/1/ctx_s/1/__text,citation_contexts/citation_context/1/ctx_s/2/_ctx_id,citation_contexts/citation_context/1/ctx_s/2/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/2/_s_id,citation_contexts/citation_context/1/ctx_s/2/_useful,citation_contexts/citation_context/1/ctx_s/2/__text,citation_contexts/citation_context/1/ctx_s/3/_ctx_id,citation_contexts/citation_context/1/ctx_s/3/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/3/_s_id,citation_contexts/citation_context/1/ctx_s/3/_useful,citation_contexts/citation_context/1/ctx_s/3/__text,citation_contexts/citation_context/1/ctx_s/4/_ctx_id,citation_contexts/citation_context/1/ctx_s/4/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/4/_s_id,citation_contexts/citation_context/1/ctx_s/4/_useful,citation_contexts/citation_context/1/ctx_s/4/__text,citation_contexts/citation_context/1/ctx_s/5/_ctx_id,citation_contexts/citation_context/1/ctx_s/5/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/5/_s_id,citation_contexts/citation_context/1/ctx_s/5/_useful,citation_contexts/citation_context/1/ctx_s/5/__text,citation_contexts/citation_context/1/_mapped,citation_contexts/citation_context/1/_ref_ids,citation_contexts/citation_context/1/_reliable,citation_contexts/citation_context/1/_s_id,citation_contexts/citation_context/1/_section_header,citation_contexts/citation_context/1/_section_number,citation_contexts/citation_context/0/ctx_ref/1/_cit_str_original,citation_contexts/citation_context/0/ctx_ref/1/_end,citation_contexts/citation_context/0/ctx_ref/1/_function,citation_contexts/citation_context/0/ctx_ref/1/_function_originally_annotated,citation_contexts/citation_context/0/ctx_ref/1/_polarity,citation_contexts/citation_context/0/ctx_ref/1/_ref_id,citation_contexts/citation_context/0/ctx_ref/1/_reliable,citation_contexts/citation_context/0/ctx_ref/1/_start,citation_contexts/citation_context/0/ctx_ref/1/_target,citation_contexts/citation_context/0/ctx_ref/1/_type,citation_contexts/citation_context/0/ctx_ref/1/__text,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1005,"Furthermore , it lends itself readily to an extension for the intensional verb case that has advantages over the widely-assumed account of Partee and Rooth 1983 .",Partee and Rooth 1983,13,CoCo-,CoCo-,UNK,26,TRUE,1,#b12,bibr,Partee and Rooth (1983),20,-2,189,N,The multiple use of semantic contributions results from viewing dependencies in f-structures as resources; in this way the one-to-one correspondence between f-structure relations and meanings is maintained.,20,-1,190,N,"The resulting account does not suffer from overgeneration inherent in other approaches, and applies equally to cases of resource sharing that do not involve coordination.",20,0,191,N,"Furthermore, it lends itself readily to an extension for the intensional verb case that has advantages over the widely-assumed account of Partee and Rooth (1983).",20,1,192,N,Here we have separated the issue of arriving at the appropriate f-structure in the syntax from the issue of deriving the correct semantics from the f-structure.,20,2,193,N,"We have argued that this is the correct distinction to make, and have given a treatment of the second issue.",20,3,194,N,A treatment of the first issue will be articulated in a future forum.,TRUE,26,TRUE,191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1024,"This indexing technique , as illustrated below , improves upon the more complex indices in Gerdemann 1991 and is closely related to OLDT-resolution Tamaki and Sato 1986 .",Gerdemann 1991,13,CoCo-,CoCo-,UNK,19,TRUE,2,#b2,bibr,Gerdemann (1991),16,-2,42,N,"The first supplies each edge in the chart with two indices, a backward index pointing to the state in the chart that the edge is predicted from, and a forward index poinfing to the states that are predicted from the edge.",16,-1,43,N,"By matching forward and backward indices, the edges that must be combined for completion can be located faster.",16,0,44,N,"This indexing technique, as illustrated below, improves upon the more complex indices in Gerdemann (1991) and is closely related to OLDT-resolution (Tamaki and Sato, 1986).",16,1,45,N,Active edge 2 resulted from active edge 1 through prediction.,16,2,46,N,The backward index of edge 2 is therefore identified with the forward index of edge,16,3,47,N,1,TRUE,"19,20",TRUE,44,Optimizations,2.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1035,"This approach improves upon earlier work on discourse structure such as Lascarides and Asher 1991 and Kehler 1994a in reducing the number of possible ambiguities ; it is also more precise than the Kamp / Hinrichs / Partee approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities , which behave differently in narrative progression .",Kehler 1994a,13,CoCo-,CoCo-,UNK,14,FALSE,2,#b7,bibr,"(Lascarides and Asher, 1991)",10,-2,115,N,An HPSG implementation of a discourse grammar Following Scha ~ Polanyi (1988),10,-1,116,N,"To allow the above-mentioned types of information to mutually constrain each other, we employ a hierarchy of rhetorical and temporal relations (illustrated in Figure 1), using the ALE system in such a way that clues such as tense and cue words work together to reduce the number of possible temporal structures.",10,0,117,N,"This approach improves upon earlier work on discourse structure such as (Lascarides and Asher, 1991) and (Kehler, 1994) in reducing the number of possible ambiguities; it is also more precise than the Kamp/Hinrichs/Partee approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities, which behave differently in narrative progression.",10,1,118,N,"Tense, aspect, rhetorical relations and temporal expressions affect the value of the RHET..",10,2,119,N,RELN type that expresses the relationship between two I)CVs: cue words are lexic,10,3,120,N,"Mly marked according to what rhetorical relation they specify, and this rel.ation is passed on to the DCU.",TRUE,"14,15",TRUE,117,TEMP_CENTER: Used for temporal centering;,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E99-1023,The IB1-IG algorithm has been able to improve the best reported F  rates for a standard data set ( 92.37 versus Ramshaw and Marcus 1995 ' s 92.03 ) .,Ramshaw and Marcus 1995,21,CoCo-,CoCo-,UNK,14,TRUE,3,#b5,bibr,"((Ramshaw and Marcus, 1995)",11,-2,117,N,"We have used the optimal experiment configurations that we had obtained from the fourth experiment series for processing the complete (Ramshaw and Marcus, 1995) data set.",11,-1,118,N,The results can be found in table 6. They are better than the results for section 15 because more training data was used in these experiments.,11,0,119,N,"Again the best result was obtained with IOB1 (F~=I =92.37) which is an im-I)rovement of the best reported F,~=1 rate for this data set ((Ramshaw and Marcus, 1995): 92.03).",11,1,120,N,"We would like to apply our learning approach to the large data set mentioned in (Ramshaw and Marcus, 1995)(Ramshaw and Marcus, 1995):",11,2,121,N,Wall Street Journal corpus sections 2-21 as training material and section 0 as test material.,11,3,122,N,With our present hardware applying our optimal experiment configuration to this data would require several months of computer time.,TRUE,14,TRUE,119,Results,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P97-1033,"Unlike other work ( e.g. 
Black et al. 1992 , 
Magerman 1995 ) , we treat the word identities as a further refinement of the POS tags ; thus we build a word classification tree for each POS tag .",Magerman 1995,21,CoCo-,CoCo-,UNK,32,TRUE,3,#b3,bibr,"(Black et al., 1992",16,-2,88,N,Figure 1 shows a POS classification tree.,16,-1,89,N,The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag.,16,0,90,N,"Unlike other work (e.g. (Black et al., 1992; Materman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag.",16,1,91,N,"This has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information.",16,2,92,N,"As well, it constrains the task of building the word classification trees since the major distinctions are captured by the POS classification tree.",,,,,,TRUE,32,TRUE,90,T editing term interruption point,UNKNOW SECTION NUMBER,"In contrast , a word-based trigram backoff model Katz 1987 built with the CMU statistical language modeling toolkit Rosenfeld 1995 achieved a perplexity of 26.13 .",Katz 1987,60,CoCo-,CoCo-,UNK,44,TRUE,48,#b15,bibr,"(Katz, 1987)",22,-2,177,N,"As can be seen, modeling the user's utterances improves POS tagging, identification of discourse markers, and word perplexity; with the POS error rate decreasing by 3.1% and perplexity by 5.3%.",22,-1,178,N,"Furthermore, adding in silence information to help detect the boundary tones and speech repairs results in a further improvement, with the overall POS tagging error rate decreasing by 8.6% and reducing perplexity by 7.8%.",22,0,179,N,"In contrast, a word-based trigram backoff model (Katz, 1987) built with the CMU statistical language modeling toolkit (Rosenfeld, 1995) achieved a perplexity of 26.13.",22,1,180,N,Thus our full language model results in 14.1% reduction in perplexity.,22,2,181,N,Table 3 gives the results of detecting intonational boundaries.,22,3,182,N,"The second column gives the results of adding the boundary tone detection to the POS model, the third column adds silence information,  Table 4: Detecting and Correcting Speech Repairs and the fourth column adds speech repair detection and correction.",TRUE,"44,45",TRUE,179,Results,7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-1026,"Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity , most notable L-TAG Schabes et al. 1988 and some versions of CG Hepple 1994 .",Schabes et al. 1988,24,CoCo-,CoCo-,UNK,45,TRUE,3,#b27,bibr,"(Schabes et al., 1988)",29,-2,142,N,6 Comparison to PSG Approaches,29,-1,143,N,"One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives.",29,0,144,N,"Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994).",29,1,145,N,"This is not necessary in our approach, which drastically reduces the search space for parsing.",29,2,146,N,"This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains.",29,3,147,N,Surface ordering is determined by the sequence of constituents associated with the root node.,TRUE,"45,46",TRUE,144,The German Clause,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hepple 1994,223,CoCo-,CoCo-,UNK,46,TRUE,209,#b13,bibr,"(Hepple, 1994)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-1052,Our assessment is supported by comparing our results to those of Core and Allen 1997 who used the unadapted DRI manual -- see Table  .,Core and Allen 1997,24,CoCo-,CoCo-,UNK,4,TRUE,5,#b2,bibr,"(Core and Allen, 1997;",2,-2,10,N,"In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes.",2,-1,11,N,A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html).,2,0,12,N,"Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997;Poesio and Traum, 1997), we have attempted to adapt it to our corpus and particular research questions.",2,1,13,N,"First we describe our corpus, and the issue of tracking agreement.",2,2,14,N,Next we describe our coding scheme and our intercoder reliability outcomes.,2,3,15,N,Last we report our findings .on tracking agreement.,TRUE,"4,5",TRUE,12,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A94-1010,"Most other work on clustering for language modeling Pereira et al. 1993 , Net et al. 1994 has addressed the problem of data sparseness by clustering words into classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed during training .",Pereira et al. 1993,24,CoCoGM,CoCoGM,UNK,6,TRUE,5,#b11,bibr,"Pereira, Tishby and Lee, 1993;",Net et al. 1994,116,CoCoGM,CoCoGM,UNK,7,TRUE,88,#b10,bibr,"Ney, Essen and Kneser, 1994)",4,0,37,N,"Most other work on clustering for language modeling (e.g. Pereira, Tishby and Lee, 1993;Ney, Essen and Kneser, 1994) has addressed the problem of data sparseness by clustering words into classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed during training.",4,1,38,N,"Thus conceptually at least, their processes are agglomerative: a large initial set of words is clumped into a smaller number of clusters.",4,2,39,N,The approach described here is quite different.,4,3,40,N,"Firstly, it involves clustering whole sentences, not words.",TRUE,"6,7",TRUE,37,Cluster-based Language Modeling,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A97-1052,"This simple automated , hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicographers Meyers et al. 1994 , who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood , culled mostly from the linguistics literature .",Meyers et al. 1994,26,CoCoGM,CoCoGM,UNK,23,TRUE,5,#b22,bibr,"(Meyers et al., 1994)",,,,,,,,,,,,13,-2,90,N,• arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output patterns for certain classes more often than others; and,13,-1,91,N,"• even a highest ranked pattern for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i.",13,0,92,N,"This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicographers (Meyers et al., 1994), who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature.",13,1,93,N,"Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing.",TRUE,23,TRUE,92,Discussion,2.3,13,2,94,N,"Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions.",13,3,95,N,We hope to exploit this information where possible at a later stage in the development of our approach.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C92-1054,This representation of mutual belief differs from the common representation in terms of an iterated conjunction Litman and Allen 1990 in that :,Litman and Allen 1990,26,CoCoGM,CoCoGM,UNK,20,TRUE,5,,bibr,[ll],,,,,,,,,,,,14,-1,47,N,"This seems intuitively plausible and means that the strength of belief depends on the strength of underlying assumptions, and that for all inference rules that depend on multiple premises, the strength of an inferred belief is the weakest of the supporting beliefs.",14,0,48,N,This representation of mutual belief differs from the common representation in terms of an iterated eonjunction [ll] in that: (1) it relocates information from mental states to the environment in which utteranees occur; (2) it allows one to represent the different kinds of evidence for mutual belief; (3) it controls reasoning when discrepancies in mutual beliefs are discovered since evidence and assumptions can be inspected; (4) it does not consist of an infinite list of statements.,,,,,,,,,,,TRUE,20,TRUE,48,"4,))",UNKNOW SECTION NUMBER,,,,,,,,,,,"This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed otherwise Litman and Allen 1990, Grosz and Sidner 1990 .",Grosz and Sidner 1990,180,CoCoGM,CoCoGM,UNK,21,TRUE,173,bibr,"[ll, 8]",15,-2,51,N,The key claim of this section is that agents monitor the effects of their utterance actions and that the next action by the addressee is taken as evidence of the effect of the speaker's utterance 4.,15,-1,52,N,"That the utterance will have the intended effect is only a hypothesis at the point where the utterance has just been made, irrespective of the intentions of the speaker.",15,0,53,N,"This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed othecwise [ll, 8].",15,1,54,N,I adopt the assumption that the participants in a dialogue are trying to achieve some purpose [7].,15,2,55,N,Some aspects of the structure of dialogue arises from the structure of these purposes and their relation to one another.,15,3,56,N,"The minimal purpose of any dialogue is that an utterance be understood, and this goal is a prerequisite to achieving other goals in dialogue, such as commitment to future action.",TRUE,21,TRUE,53,Inference of Understanding,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C94-1056,"First , unlike conventional word-frequency-based abstract generation systems Kuhn 1958 , the generated abstract is consistent with the original text in that the connectives between sentences in the abstract reflect their relation in the original text .",Kuhn 1958,43,CoCoGM,CoCoGM,UNK,0,TRUE,5,,bibr,[Kuhn 58]),,,,,,,,,,,,0,-1,5,N,"Abstract generation is, like Machine Translation, one of the ultimate goal of Natural Language Processing.",0,0,6,N,"IIowever, since conventional word-frequencybased abstract generation systems(e.g. [Kuhn 58]) are lacking in inter-sentential or discourse-structural analysis, they are liable to generate incoherent abstracts.",0,1,7,N,"On the other hand, conventional knowledge or script-based abstract generation systems(e.g.",0,2,8,N,"[behnert 801, [Fum 86]), owe their success to the li,nitation of the domain, and cannot be applied to document with varied subjects, such ,as popular scientific magazine.",TRUE,0,TRUE,6,I N T R O D U C T I O N,2,0,3,9,N,"To realize a domain-independent abstract generation system, a computational theory for analyzing linguistic discourse structure and its practical procedure must be established.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C94-1082,"In the central role of heads , LHIP resembles parsers devised by Kay 1989 and van Noord 1991 ; in other respects , including the use which is made of heads , the approaches are rather different , however .",Kay 1989,43,CoCoGM,CoCoGM,UNK,5,TRUE,5,#b4,bibr,Kay (1989),van Noord 1991,96,CoCoGM,CoCoGM,UNK,6,TRUE,84,#b7,bibr,Noord (1991),4,-2,41,N,"In particular, nothing requires that a head of a rule should share any information with the LItS item, although in practice it often will.",4,-1,42,N,"Heads serve as anchor-points in the input string around which islands may be formed, and are accordingly treated before non-head items (RHS items are re-ordered during compilation-see below).",4,0,43,N,"In the central role of heads, LtIIP resembles parsers devised by Kay (1989) and van Noord (1991); in other respects, including the use which is made of heads, the approaches are rather different, however.",,,,,,TRUE,"5,6",TRUE,43,LHIP Overview,1,,,,,,,,,,,Recent proposals include the use of probabilistic methods Briscoe and Carroll 1993 and large robust deterministic systems like Hindle 's Fidditch Hindle 1989 .,Briscoe and Carroll 1993,94,CoCoGM,CoCoGM,UNK,8,TRUE,68,bibr,"Briseoe and Carroll, 1993)",6,-1,162,N,The ability to deal with large amomlts of possibly ill-formed text is one of the principal objectives of current NLP research.,6,0,163,N,"Recent proposals include the use of probabilistic methods (see e.g. Briseoe and Carroll, 1993) and large robust deterministic systems like Hindle's Fidditch (Hindle, 1989).",6,1,164,N,4 Experience so far suggests that systems like LIIIP may in the right circumstances provide an alternative to these approaches.,6,2,165,N,"It combines the advantages of Prolog-interpreted DCGs (ease of modification, parser output suitable for direct use by other programs, etc.) with the ability to relax tile adjacency constraints of that form&llsm in a flexible and dynamic manner.",6,3,166,N,"LIHP is based on the assumption that partial results can be useful (often much more useful than no result at all), and that an approximation to complete coverage is more useful when it comes with indications of how approximate it is.",,,,,,TRUE,"8,9",TRUE,163,Discussion,5,Hindle 1989,171,CoCoGM,CoCoGM,UNK,9,TRUE,157,#b2,bibr,"(Hindle, 1989)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C94-2199,"The existence of such a transformation is known: in Dymetman 1992a,Dymetman 1992, we have recently introduced a ``Generalized Greibach Normal Form'' (GGNF) for DCGs, which leads to termination of top-down interpretation in the OP case .",Dymetman 1992,54,CoCoGM,CoCoGM,UNK,1,FALSE,5,#b0,bibr,"[1,",,,,,,,,,,,,1,-2,12,N,2,1,-1,13,N,Our aim in this paper is to propose a simple transformation lbr an arbitrary OP DCG putting it into a form which leads to the completeness of the direct top-down interpretation by the standard Prolog interpreter: parsing is guaranteed to enumerate all solutions to the parsing problem and terminate.,1,0,14,N,"The e.xistence of such a transformation is kuown: in [1,2], we have recently introduced a ""Generalized Greibach Normal Form"" (GGNF) for DCGs, which leads to termination of top-down interpretation in the OP case.",1,1,15,N,"lIowever, the awdlable presentation of the GGNF transformation is rather complex (it involves an algebraic study of the fixpoints of certain equational systems representing grammars.).",TRUE,"1,2",TRUE,14,Motivation,1,1,2,16,N,"Our aim here is to present a related, but much simpler, transformation, which from a theoretical viewpoint performs somewhat less than the GGNF transformation (it; involves some encoding of the initial DCG, which the (~GNF does not, and it only handles oflline-parsable grammars, while the GGNF is defined for arbitrary DCGs), a but in practice is extremely easy to implement and displays a comparable behavior when parsing with an OP grammar.",1,3,17,N,3'he transformation consists of two steps: (1) emptyproduction elimination and (2) left-recursion elimination.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C94-2202,Our work is related to research by Heid and Raab 1989 .,Heid and Raab 1989,54,CoCoGM,CoCoGM,UNK,7,TRUE,4,#b2,bibr,"(Heid and Raab, 1989)",,,,,,,,,,,,2,-2,15,N,"word sense disambiguation, information retrieval, natural language generation and so on.",2,-1,16,N,"Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992), (Pnstejovsky et al., 1992), (Smadja and McKeown, 1990) etc.).",2,0,17,N,"However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988), machine translation, (Heid and Raab, 1989)) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989), (Krenn and Erbach, to appear)).",2,1,18,N,We have been concerned with investigating the lexical .,TRUE,"6,7,8,9",TRUE,17,Description of the Problem,1,2,2,19,N,"['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates.",2,3,20,N,"Our work is related to research by (Heid and Raab, /989).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C98-2187,"Several restrictions on the adjunction operation for TAG have been proposed in the literature Schabes and Waters 1993 , Schabes and Waters 1995 Rogers 1994 .",Schabes and Waters 1993,62,CoCoGM,CoCoGM,UNK,4,TRUE,4,,bibr,"(Schabes and Waters, 1993;",Schabes and Waters 1995,145,CoCoGM,CoCoGM,UNK,5,TRUE,120,,bibr,"Schabes and Waters, 1995)",4,-2,13,N,•,4,-1,14,N,We provide evidence that the proposed subclass still captures the vast majority of TAG analyses that have been currently proposed for the syntax of English and of several other languages.,4,0,15,N,"Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993;Schabes and Waters, 1995) (Rogers, 1994).",4,1,16,N,"Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG).",TRUE,"4,5,6",TRUE,15,Introduction,1,4,2,17,N,"For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above.",,,,,,"Our restriction is fundamentally different from those in Schabes and Waters 1993 , Schabes and Waters 1995 and Rogers 1994 , in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times , so long as they only adjoin at one place in each others ' spines .",Schabes and Waters 1993,83,CoCoGM,CoCoGM,UNK,8,TRUE,57,bibr,"(Schabes and Waters, 1993;",6,-2,32,N,The above restriction does not in any way constrain adjunction at nodes that are not in the st)ine of ass auxiliary tree.,6,-1,33,N,"Similarly, there is no restriction on the adjunction of left or right trees at the spines of wrapping trees.",6,0,34,N,"Our restriction is fundamentally different from those in (Schabes and Waters, 1993;Schabes and Waters, 1995) and (Rogers, 1994), in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times, so long as they only adjoin at one place its each others' spines.",6,1,35,N,"Rogers, in contrast, restricts the nesting of wrapping auxiliaries to a number of times bounded by the size of the grammar, and Sehabes and Waters forbid wrapping auxiliaries altogether, at any node in the grammar.",6,2,36,N,"We now focus on the recognition problem, and informally discuss the computational advantages that arise in this task when a TAG obeys the above restriction.",6,3,37,N,These ideas are fl)rmally developed in the next section.,TRUE,"8,9,10",TRUE,34,Overview,2,Schabes and Waters 1995,108,CoCoGM,CoCoGM,UNK,9,TRUE,83,,bibr,"Schabes and Waters, 1995)",Rogers 1994,160,CoCoGM,CoCoGM,UNK,6,TRUE,146,bibr,"(Rogers, 1994)",Rogers 1994,127,CoCoGM,CoCoGM,UNK,10,TRUE,113,bibr,"(Rogers, 1994)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1017,"The main difference between our approach and that of Milward 1992 , Milward 1994a is that it is based on a more expressive grammar formalism , Applicative Categorial Grammar , as opposed to Lexicalised Dependency Grammar .",Milward 1994a,62,CoCoGM,CoCoGM,UNK,13,FALSE,4,#b13,bibr,Milward (1992,,,,,,,,,,,,9,-2,51,N,There are however problems with this kind of approach when features are considered (see e.g. Vijay-Shanker 1992).,9,-1,52,N,"This provides a state-transition or dynamic model of processing, with each state being a pair of a syntactic type and a semantic value.",9,0,53,N,"The main difference between our approach and that of Milward (1992Milward ( , 1994 is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar.",9,1,54,N,"Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a function of a function, and given the type (n/n)/(n/n) when used as an adjectival modifier).",TRUE,"13,14",TRUE,53,Introduction,UNKNOW SECTION NUMBER,9,2,55,N,"The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions, and in providing one kind of robust parsing: the parser never fails until the last word, since there could always be a final word which is a function over all the constituents formed so far.",9,3,56,N,"However, there is a corresponding problem of far greater non-determinism, with even unambiguous words allowing many possible transitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1020,"This differs from previous approaches Finch and Chater 1992 , Schuetze 1993 in which left and right context vectors of a word are always used in one concatenated vector .",Finch and Chater 1992,66,CoCoGM,CoCoGM,UNK,24,TRUE,4,#b14,bibr,"(Finch and Chater, 1992;",Schuetze 1993,77,CoCoGM,CoCoGM,UNK,25,TRUE,62,#b22,bibr,"Schfitze, 1993)",15,-2,124,N,Generalized left context vectors were derived by an analogous procedure using word-based right context vectors.,15,-1,125,N,Note that the information about left and right is kept separate in this computation.,15,0,126,N,"This differs from previous approaches (Finch and Chater, 1992;Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector.",15,1,127,N,There are arguably fewer different types of right syntactic contexts than types of syntactic categories.,TRUE,"24,25",TRUE,126,Generalized context vectors,3.3,15,2,128,N,"For example, transitive verbs and prepositions belong to different syntactic categories, but their right contexts are virtually identical in that they require a noun phrase.",15,3,129,N,This generalization could not be exploited if left and right context were not treated separately.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1024,"By combining the off-line optimization process with a mixed bottom-up / top-down evaluation strategy , we can refrain from a complete reformulation of the grammar as , for example , in Minnen et al. 1995 .",Minnen et al. 1995,68,CoCoGM,CoCoGM,UNK,18,TRUE,4,#b10,bibr,Minnen et al. (1995),,,,,,,,,,,,15,-2,37,N,"A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing.",15,-1,38,N,"Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule.",15,0,39,N,"By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995).",,,,,,TRUE,18,TRUE,39,Advanced Earley Generation,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E95-1030,Atwell 1987 and Church 1989 have used this approach .,Atwell 1987,66,CoCoGM,CoCoGM,UNK,6,TRUE,4,,bibr,"(Atwell, 1987)",Church 1989,33,CoCoGM,CoCoGM,UNK,7,TRUE,19,,bibr,"(Church, 1989)",3,-2,64,N,"In the same way that tags are allocated to words, or to punctuation marks, they can represent the boundaries of syntactic constituents, such as noun phrases and verb phrases.",3,-1,65,N,"Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do.",3,0,66,N,"(Atwell, 1987) and (Church, 1989) have used this approach.",3,1,67,N,"If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994).",TRUE,"6,7",TRUE,66,Representing syntactic boundary markers,UNKNOW SECTION NUMBER,3,2,68,N,"Our approach uses a similar concept, but differs in that embedded syntactic constituents are detected one at a time in separate steps.",3,3,69,N,There are only 2 hypertags -the opening and closing brackets marking the possible location(s) of the syntactic constituent in question.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E99-1023,We have compared four complete and three partial data representation formats for the baseNP recognition task presented in Ramshaw and Marcus 1995 .,Ramshaw and Marcus 1995,75,CoCoGM,CoCoGM,UNK,6,TRUE,4,#b5,bibr,"(Ramshaw and Marcus, 1995)",,,,,,,,,,,,3,0,24,N,"We have compared four complete and three partial data representation formats for the baseNP recognition task presented in (Ramshaw and Marcus, 1995).",3,1,25,N,The four complete formats all use an I tag for words that are inside a baseNP and an 0 tag for words that are outside a base,3,2,26,N,NP.,3,3,27,N,They differ IOB1 O  I  I  O  I  I B O  I  O O O  I  I B  I O  IOB2 O B  I  O B  I  B O B O O O B  I  B  I  O  IOE1 O  I  I  O  I  E  I  O  I  O O O  I  E  I  I  O  IOE2 O  I E O  I  E E O E O O O  I  E  I  E O  IO I O  I  I  O  I  I  I  O  I  O O O  I  I  I  I,TRUE,6,TRUE,24,Data representation,2.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J00-2016,For the French text the tag set described in Chanod and Tapanainen 1994 with 37 different tags is used .,Cutting et al. 1992,83,CoCoGM,CoCoGM,UNK,2,FALSE,4,,bibr,"(Columbus, 1993;",,,,,,,,,,,,1,-1,0,N,"Dordrecht: Kluwer Academic Publishers (Text, speech and language technology series, edited by Nancy Ide and Jean V~ronis, volume 11), 1999, xvii+304 pp;hardbound, ISBN 0-7923-6055-9, $128.00, £79, Dfl 240.00",1,0,1,N,"This volume is a collection of important papers selected from the proceedings of the first three Workshops on Very Large Corpora (Columbus, 1993;Kyoto, 1994;Cambridge, MA, 1995) and the 1995 workshop entitled From Text to Tags (Dublin).",1,1,2,N,"The editors write in their introduction: ""The success of these workshops was in some measure a reflection of the growing popularity of corpus-based methods in the NLP community.",1,2,3,N,"But first and foremost, it was due to the fact that the workshops attracted so many high-quality papers.",TRUE,"2,3,4",TRUE,1,"Dordrecht: Kluwer Academic Publishers (Text, speec",UNKNOW SECTION NUMBER,1,3,4,N,"The importance of this material for the field is such that it deserves to be made more readily available than harder-to-find or out-ofprint workshop proceedings.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P93-1024,"While it may be worthwhile to base such a model on preexisting sense classes Resnik 1992 , in the work described here we look at how to derive the classes directly from distributional data .",Resnik 1992,88,CoCoGM,CoCoGM,UNK,1,TRUE,4,,bibr,"(Resnik, 1992)",,,,,,,,,,,,1,-2,15,N,"His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association.",1,-1,16,N,"Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves.",1,0,17,N,"While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data.",1,1,18,N,"More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w.",TRUE,1,TRUE,17,INTRODUCTION,UNKNOW SECTION NUMBER,1,2,19,N,"Most other class-based modeling techniques for natural language rely instead on ""hard"" Boolean classes (Brown et al., 1990).",1,3,20,N,"Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above.",Most other class-based modeling techniques for natural language rely instead on `` hard '' Boolean classes Brown et al. 1990 .,Brown et al. 1990,123,CoCoGM,CoCoGM,UNK,2,TRUE,103,bibr,"(Brown et al., 1990)",2,-2,17,N,"While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data.",2,-1,18,N,"More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w.",2,0,19,N,"Most other class-based modeling techniques for natural language rely instead on ""hard"" Boolean classes (Brown et al., 1990).",2,1,20,N,"Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above.",2,2,21,N,Our approach avoids both problems.,,,,,,TRUE,2,TRUE,19,INTRODUCTION,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,#b0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P94-1008,"Instead , we briefly compare the current work to three previous studies that explicitly tie ellipsis resolution to an account of discourse structure and coherence , namely our previous account Kehler 1993b and the accounts of Pruest 1992 and Asher 1993 .",Pruest 1992,88,CoCoGM,CoCoGM,UNK,27,FALSE,4,#b7,bibr,"(Kehler, 1993b)",Asher 1993,157,CoCoGM,CoCoGM,UNK,28,TRUE,145,#b0,bibr,Asher (1993),20,-2,174,N,"The literature on ellipsis and event reference is voluminous, and so we will not attempt a comprehensive comparison here.",20,-1,175,N,"Instead, we briefly compare the current work to three previous studies that explicitly tie ellipsis 14Sag and Hankamer claim that all such cases of VPellipsis require syntactic antecedents, whereas we suggest that in Coherent Situation relations VP-eUipsis operates more like their Model-Interpretive Anaphora, of which do it is an example.",20,0,176,N,"resolution to an account of discourse structure and coherence, namely our previous account (Kehler, 1993b) and the accounts of Priist (1992) and Asher (1993).",20,1,177,N,"In Kehler (1993b), we presented an analysis of VPellipsis that distinguished between two types of relationship between clauses, parallel and non-parallel.",TRUE,"27,28",TRUE,176,Relationship to Past Work,UNKNOW SECTION NUMBER,20,2,178,N,An architecture was presented whereby utterances were parsed into propositional representations which were subsequently integrated into a discourse model.,20,3,179,N,"It was posited that VP-ellipsis could access either propositional or discourse model representations: in the case of parallel constructions, the source resided in the propositional representation; in the case of non-parallel constructions, the source had been integrated into the discourse model.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P95-1007,"In the test set used here and in that of Resnik 1993 , the proportion of left-branching compounds is 67 % and 64 % respectively .",Resnik 1993,91,CoCoGM,CoCoGM,UNK,25,TRUE,4,#b17,bibr,Resnik (1993),,,,,,,,,,,,18,-2,82,N,Figure 1: Two analysis models and the associations they compare of left and right-branching compounds.,18,-1,83,N,"Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time).",18,0,84,N,"In the test set used here and in that of Resnik (1993), the proportion of leftbranching compounds is 67% and 64% respectively.",18,1,85,N,"In contrast, the adjacency model appears to predict a proportion of 50%.",TRUE,25,TRUE,84,Noun Compound Analysis,1.3,18,2,86,N,"The dependency model has also been proposed by Kobayasi et al (1994) for analysing Japanese noun compounds, apparently independently.",18,3,87,N,"Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words).",The equations presented above for the dependency model differ from those developed in Lauer and Dras 1994 in one way .,Lauer and Dras 1994,107,CoCoGM,CoCoGM,UNK,32,FALSE,86,bibr,Lauer and Dras (1994),25,-2,137,N,"In such cases, we observe that the test instance itself provides the information that the event t2 --~ t3 can occur and we recalculate the ratio using Pr(t2 ---* t3) = k for all possible categories t2,t a where k is any non-zero constant.",25,-1,138,N,"However, no correction is made to the probability estimates for Pr(tl --~ t2) and Pr(Q --* t3) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above.",25,0,139,N,The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994)Lauer and Dras (1994) in one way.,25,1,140,N,"There, an additional weighting factor (of 2.0) is used to favour a left-branching analysis.",25,2,141,N,This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often.,25,3,142,N,"Also, the work reported in Lauer and Dras (1994) uses simplistic estimates of the probability of a word given its thesaurus category.",TRUE,"32,33",TRUE,139,Analysing the Test Set,2.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,#b10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P95-1019,"Walker 1994c described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost , inference cost , and cost of memory retrieval .",Walker 1994c,91,CoCoGM,CoCoGM,UNK,8,TRUE,4,#b33,bibr,Walker (1994),,,,,,,,,,,,3,-2,17,N,Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses.,3,-1,18,N,"They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected.",3,0,19,N,"Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval.",3,1,20,N,"However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, ff so, selects appropriate evidence from the system's private beliefs to support the claim.",TRUE,8,TRUE,19,Related Work,2,3,2,21,N,Caswey et al.,3,3,22,N,"(Cawsey et al., 1993;Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P95-1029,"Also , whereas Park 1992 requires careful consideration of handling of determiners with coordination , here such sentences are handled just like any others .",Park 1992,92,CoCoGM,CoCoGM,UNK,23,TRUE,4,#b10,bibr,"(Park, 1992)",,,,,,,,,,,,14,-2,161,N,"For example, the following query corresponds to rule (7b).",14,-1,162,N,"Note also that the use of the same bound variable names obj and sub causes no difficulty since the use of scoped-constants, meta-level H-reduction, and higher-order unification is used to access and manipulate the inner terms.",14,0,163,N,"Also, whereas (Park, 1992) requires careful consideration of handling of determiners with coordination, here such sentences are handled just like any others.",14,1,164,N,"For example, the sentence Mary gave every dog a bone and some policeman a flower results in the LF",TRUE,23,TRUE,163,Implementation of Coordination,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P96-1033,Generation with the resulting grammar can be compared best with head corner generation Shieber et al. 1990 ( see next section ) .,Shieber et al. 1990,100,CoCoGM,CoCoGM,UNK,22,TRUE,4,#b10,bibr,"(Shieber et al., 1990",,,,,,,,,,,,19,-2,127,N,"This filter optimization is reminiscent of computing the deterministic closure over the magic part of a compiled grammar (DSrre, 1993) at compile time.",19,-1,128,N,"Performing this optimization throughout the magic part of the grammar in figure 2 not only leads to a more succinct grammar, but brings about a different processing behavior.",19,0,129,N,"Generation with the resulting grammar can be compared best with head corner generation (Shieber et al., 1990) (see next section).",,,,,,TRUE,22,TRUE,129,Redundant Filtering Steps,3.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P96-1043,This estimation of the rule value in fact resembles that used by Tzoukermann et al. 1995 for scoring POS - disambiguation rules for the French tagger .,Tzoukermann et al. 1995,100,CoCoGM,CoCoGM,UNK,10,TRUE,4,#b6,bibr,"(Tzoukermann et al., 1995)",,,,,,,,,,,,9,-2,96,N,"Even if one rule has a high estimate but that estimate was obtained over a small sample, another rule with a lower estimate but over a large sample might be valued higher.",9,-1,97,N,Note also that since/3 itself is smoothed we will not have zeros in positive (/3) or negative (1 -/3) outcome probabilities.,9,0,98,N,"This estimation of the rule value in fact resembles that used by (Tzoukermann et al., 1995) for scoring pos-disambiguation rules for the French tagger.",9,1,99,N,The main difference between the two functions is that there the z value was implicitly assumed to be 1 which corresponds to the confidence of 68%.,TRUE,10,TRUE,98,Rule Scoring Phase,2.2,9,2,100,N,A more standard approach is to adopt a rather high confidence value in the range of 90-95%.,9,3,101,N,We adopted 90% confidence for which z(1-0.90)/2 = z0.05 = 1.65.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P96-1058,"Note that in this paper *restrictor* specifies the features to be removed by  , whereas in Shieber 1985 , Shieber 1992 restrictor specifies the features to be retained by restriction which is equivalent to  .",Shieber 1985,100,CoCoGM,CoCoGM,UNK,7,TRUE,4,#b5,bibr,"(Shieber, 1985",Shieber 1992,84,CoCoGM,CoCoGM,UNK,8,TRUE,68,#b7,bibr,"(Shieber, , 1992",5,-2,17,N,"The filtering function p is similar to (Shieber, 1992): p(D) returns a copy of D in which some features may be removed.",5,-1,18,N,Note that in this paper .restrictor.,5,0,19,N,"specifies the features to be removed by p, whereas in (Shieber, 1985(Shieber, , 1992 restrictor specifies the features to be retained by restriction which is equivalent to p.",,,,,,TRUE,"7,8",TRUE,19,Notation,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-1014,"It focusses on extracting linguistic properties , as compared to e.g. general concept learning 
Hahn et al. 1996 .",Hahn et al. 1996,105,CoCoGM,CoCoGM,UNK,3,TRUE,4,#b4,bibr,"(Hahn, Klenner & Schnattinger 1996)",,,,,,,,,,,,1,-2,13,N,We thank James Kilbury and members of the B3 group for fruitful discussion.,1,-1,14,N,tences.,1,0,15,N,"It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996).",1,1,16,N,"Unlike Erbach (1990), however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure.",TRUE,3,TRUE,15,Introduction,1,1,2,17,N,"Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.",1,3,18,N,The following three goals serve to structure our model.,"Unlike 
Erbach 1990 , however , it is not confined to simple morpho-syntactic information but can also handle selectional restrictions , semantic types and argument structure .",Erbach 1990,20,CoCoGM,CoCoGM,UNK,4,TRUE,7,bibr,Erbach (1990),2,-2,14,N,tences.,2,-1,15,N,"It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996).",2,0,16,N,"Unlike Erbach (1990), however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure.",2,1,17,N,"Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.",2,2,18,N,The following three goals serve to structure our model.,2,3,19,N,"It should i) incorporate a gradual, information-based conceptualization of ""unknownness"".",TRUE,4,TRUE,16,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,#b3,"Finally , while statistical approaches like 
Brent 1991 can gather e.g. valence information from large corpora , we are more interested in full grammatical processing of individual sentences to maximally exploit each context .",Brent 1991,55,CoCoGM,CoCoGM,UNK,5,TRUE,43,#b1,bibr,Brent (1991),3,-2,15,N,"It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996).",3,-1,16,N,"Unlike Erbach (1990), however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure.",3,0,17,N,"Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.",3,1,18,N,The following three goals serve to structure our model.,3,2,19,N,"It should i) incorporate a gradual, information-based conceptualization of ""unknownness"".",3,3,20,N,"Words are not unknown as a whole, but may contain unlmown, i.e. revisable pieces of information.",TRUE,5,TRUE,17,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-1026,"Recently , Kruijff 1997 has given a categorial-style formulation of these ordering rules .",Kruijff 1997,105,CoCoGM,CoCoGM,UNK,8,TRUE,4,#b18,bibr,Kruijff (1997),,,,,,,,,,,,6,-2,18,N,"Functional Generative Description (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree.",6,-1,19,N,This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization.,6,0,20,N,"Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules.",6,1,21,N,"He assumes associative categorial operators, permuting the arguments to yield the surface ordering.",TRUE,8,TRUE,20,Word Order in DG,2,6,2,22,N,"One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure~ which stratifies the theory and makes incremental processing difficult.",6,3,23,N,"Meaning-Text Theory (Melc'fik, 1988) assumes seven strata of representation.","This property is shared by the proposal of Reape 1993 to associate HPSG signs with sequences of constituents , also called word order domains .",Reape 1993,55,CoCoGM,CoCoGM,UNK,47,TRUE,43,bibr,Reape (1993),30,-2,144,N,"Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994).",30,-1,145,N,"This is not necessary in our approach, which drastically reduces the search space for parsing.",30,0,146,N,"This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains.",30,1,147,N,Surface ordering is determined by the sequence of constituents associated with the root node.,30,2,148,N,"The order domain of a mother node is the sequence union of the order domains of the daughter nodes, which means that the relative order of elements in an order domain is retained, but material from several domains may be interleaved, resulting in discontinuities.",30,3,149,N,Whether an order domain allows interleaving with other domains is a parameter of the constituent.,TRUE,47,TRUE,146,The German Clause,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,#b26,"We may also compare our approach with the projection architecture of LFG Kaplan and Bresnan 1982 , Kaplan 1995 .",Kaplan and Bresnan 1982,97,CoCoGM,CoCoGM,UNK,48,TRUE,73,#b17,bibr,"(Kaplan & Bresnan, 1982;",31,-2,152,N,"For example, two projections are required for an NP, the lower one for the continuous material (determiner, adjective, noun, genitival and prepositional attributes) and the higher one for the possibly discontinuous relative clause.",31,-1,153,N,This dependence of hierarchical structure on ordering is absent from our proposal.,31,0,154,N,"We may also compare our approach with the projection architecture of LFG (Kaplan & Bresnan, 1982;Kaplan, 1995).",31,1,155,N,"There is a close similarity of the LFG projections (c-structure and f-structure) to the dimensions used here (order domain structure and dependency tree, respectively).",31,2,156,N,"C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements.",31,3,157,N,"What is more, these projections or dimensions are linked in both accounts by'an element-wise mapping.",TRUE,"48,49",TRUE,154,The German Clause,5,Kaplan 1995,110,CoCoGM,CoCoGM,UNK,49,TRUE,97,#b16,bibr,"Kaplan, 1995)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-1052,Our assessment is supported by comparing our results to those of Core and Allen 1997 who used the unadapted DRI manual -- see Table  .,Poesio and Traum 1997,107,CoCoGM,CoCoGM,UNK,5,TRUE,4,#b9,bibr,"Poesio and Traum, 1997)",,,,,,,,,,,,2,-2,10,N,"In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes.",2,-1,11,N,A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html).,2,0,12,N,"Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997;Poesio and Traum, 1997), we have attempted to adapt it to our corpus and particular research questions.",2,1,13,N,"First we describe our corpus, and the issue of tracking agreement.",TRUE,"4,5",TRUE,12,Introduction,1,2,2,14,N,Next we describe our coding scheme and our intercoder reliability outcomes.,2,3,15,N,Last we report our findings .on tracking agreement.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-2176,"For example , picking one among several equivalent ( or nearly equivalent ) constructions is a form of lexical choice ( e.g. , "" The Utah Jazz handed the Boston Celtics a defeat "" vs. "" The Utah Jazz defeated the Boston Celtics "" Robin 1994 ) .",Robin 1994,108,CoCoGM,CoCoGM,UNK,2,TRUE,4,#b10,bibr,"(Robin, 1994)",,,,,,,,,,,,2,-2,46,N,"Text generation usually involves lexical choicethat is, choosing one way of referring to an entity over another.",2,-1,47,N,Lexical choice refers to a variety of decisions that have to made in text generation.,2,0,48,N,"For example, picking one among several equivalent (or nearly equivalent) constructions is a form of lexical choice (e.g., ""The Utah Jazz handed the Boston Celtics a de fear' vs. ""The Utah Jazz defeated the Boston Celtics"" (Robin, 1994)).",2,1,49,N,We are interested in a different aspect of the problem: namely learning the rules that can be used for automatically selecting an appropriate description of an entity in a specific  context.,TRUE,2,TRUE,48,Problem Description,2,2,2,50,N,"To be feasible and scaleable, a technique for solving a particular case of the problem of lexicai choice must involve automated learning.",2,3,51,N,It is also useful if the technique can specify enough constraints on the text to be generated so that the number of possible surface realizations that match the semantic constraints is reduced significantly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W94-0208,"The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched ; both psychologists Cutler and Carter 1987 , Cutler and Butterfield 1992 and designers of speech-recognition systems Church 1987 have examined this problem .",Cutler and Carter 1987,108,CoCoGM,CoCoGM,UNK,0,TRUE,4,,bibr,"Cutler & Carter, 1987;",Cutler and Butterfield 1992,209,CoCoGM,CoCoGM,UNK,2,TRUE,200,,bibr,"(I, 1992)",0,-2,23,N,',0,-1,24,N,"Fo date, psychologists have focused on two aspects of the speech segmentation problem.",0,0,25,N,"The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched; both psychologists (e.g., Cutler & Carter, 1987;Cutler & Butterliel (I, 1992) and designers of speech-recognition systems (e.g., (]hur (:h, 1987) have examined I~his problem.",0,1,26,N,"However, the problem we examined is dilferent---we want to know how infants segment speech before knowing which phonemic se-qllel",TRUE,"0,1,2,3",TRUE,25,INTRODUCTION,UNKNOW SECTION NUMBER,0,2,27,N,"W,('s form words.",0,3,28,N,"'1'he second aspect psychologists liaw~ focnsed (ill is the lirobleln of dcternihiilig the ill['Orluatioll SOllr(:(~s t() which ilifants are SCllSil,ive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Church 1987,277,CoCoGM,CoCoGM,UNK,3,TRUE,267,bibr,"(:h, 1987)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W97-0313,"Our approach was motivated by Yarowsky's word sense disambiguation algorithm Yarowsky 1992 and the notion of statistical salience , although our system uses somewhat different statistical measures and techniques .",Yarowsky 1992,115,CoCoGM,CoCoGM,UNK,3,TRUE,4,#b12,bibr,"(Yarowsky, 1992)",,,,,,,,,,,,2,-2,25,N,"Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish).",2,-1,26,N,"Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category.",2,0,27,N,"Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques.",2,1,28,N,We begin with a small set of seed words for a category.,TRUE,3,TRUE,27,Generating a Semantic Lexicon,UNKNOW SECTION NUMBER,2,2,29,N,"We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well.",2,3,30,N,"As an example, the seed word lists used in our experiments are shown below.",Note that our context window is much narrower than those used by other researchers Yarowsky 1992 .,Yarowsky 1992,99,CoCoGM,CoCoGM,UNK,4,TRUE,83,bibr,"(Yarowsky, 1992)",3,-2,49,N,"We collected only nouns under the assumption that most, if not all, true category members would be nouns3 .",3,-1,50,N,The context windows do not cut across sentence boundaries.,3,0,51,N,"Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992).",3,1,52,N,We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category.,3,2,53,N,"Given the context windows for a category, we compute a category score for each word, which is essentially the conditional probability that the word appears in a category context.",3,3,54,N,The category score of a word W for category C is defined as:,TRUE,4,TRUE,51,Generating a Semantic Lexicon,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,#b12,"Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context ( e.g. , Granger 1977 , Carbonell 1979 , Jacobs and Zernik 1988 , Hastings and Lytinen 1994 ) .",Granger 1977,153,CoCoGM,CoCoGM,UNK,9,TRUE,138,#b5,bibr,"(Granger, 1977;",8,-2,197,N,But there is no question that semantic knowledge is essential for many problems in natural language processing.,8,-1,198,N,"Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically.",8,0,199,N,"Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977;Carbonell, 1979;Jacobs and Zernik, 1988;Hastings and Lytinen, 1994)).",8,1,200,N,"Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989;Cardie, 1993)).",8,2,201,N,"Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.",8,3,202,N,"To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge.",TRUE,"9,10,11,12",TRUE,199,Conclusions,5,Carbonell 1979,169,CoCoGM,CoCoGM,UNK,10,TRUE,153,#b2,bibr,"Carbonell, 1979;",Jacobs and Zernik 1988,193,CoCoGM,CoCoGM,UNK,11,TRUE,169,#b7,bibr,"Jacobs and Zernik, 1988;",Hastings and Lytinen 1994,220,CoCoGM,CoCoGM,UNK,12,TRUE,193,#b6,bibr,"Hastings and Lytinen, 1994)","Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts ( e.g. , Berwick 1989 , Cardie 1993 ) .",Berwick 1989,142,CoCoGM,CoCoGM,UNK,13,TRUE,127,#b0,bibr,"(Berwick, 1989;",Cardie 1993,155,CoCoGM,CoCoGM,UNK,14,TRUE,142,#b3,bibr,"Cardie, 1993)",9,-2,198,N,"Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically.",9,-1,199,N,"Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977;Carbonell, 1979;Jacobs and Zernik, 1988;Hastings and Lytinen, 1994)).",9,0,200,N,"Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989;Cardie, 1993)).",9,1,201,N,"Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.",9,2,202,N,"To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge.",9,3,203,N,The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing.,TRUE,"13,14",TRUE,200,Conclusions,5
W97-1307,"The present algorithm thus provides an interesting case of what happens with extremely poor syntactic input , even poorer than in Kennedy and Boguraev 1996 's system .",Kennedy and Boguraev 1996,115,CoCoGM,CoCoGM,UNK,16,TRUE,5,#b9,bibr,Kennedy and Boguraev's (1996),,,,,,,,,,,,8,-2,69,N,The nondestructive option has been implemented in a more recent system.,8,-1,70,N,"These basic steps of ""COLLECT, FILTER, and ORDER by salience"" are analogous to Lappin and Leass's (1994) pronoun resolution algorithm, but each step in FASTUS relies on considerably poorer syntactic input.",8,0,71,N,"The present algorithm thus provides an interesting case of what happens with extremely poor syntactic input, even poorer than in Kennedy and Boguraev's (1996) system.",8,1,72,N,This comparison will be discussed later.,TRUE,16,TRUE,71,• Sort Consistency:,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W98-0504,"Recently , 
Kruijff 1997 has given a categorial-style formulation of these ordering rules .",Kruijff 1997,116,CoCoGM,CoCoGM,UNK,10,TRUE,5,,bibr,Kruijff (1997),,,,,,,,,,,,8,-2,31,N,"Functional Generative Description (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree.",8,-1,32,N,This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization.,8,0,33,N,"Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules.",8,1,34,N,"He assumes associative categorial operators, permuting the arguments to yield the surface ordering.",TRUE,10,TRUE,33,I I,UNKNOW SECTION NUMBER,8,2,35,N,"One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary categorial operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult.",8,3,36,N,"Meaning-Text Theory (Melc'flk, 1988) assumes seven strata of representation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W98-1111,"There is another approach to language identification , which has a certain amount in common with ours , described in a patent by Martino and Paulsen 1996 .",Paulsen 1996,120,CoCoGM,CoCoGM,UNK,8,TRUE,5,#b4,bibr,Martino and Paulsen (1996),,,,,,,,,,,,5,-2,129,N,The classification algorithm described above was originally developed in response to Sibun and Spitz's work.,5,-1,130,N,"There is another approach to language identification, which has a certain   Incorrect  1560  6  128  7  37  9  18  2  5  1  5  1  2  0  2  0  1  0  3  0  1  0  2  0  2  0  1  0  7  0   All  1566  135  46  20  6  6  2  2  1  3  1  2  2 1 7  Incorrect  173  0  22  0  31  1  45  3  34  4  65  8  73  4  83  3  84  3  89  2  84  0  128  1  131  0  175  1  253  0   All  173  22  32  48  38  73  77  86  87  91  84  129  131  176  253   Table",5,0,131,N,"6: Categories remaining at end of input (genre identification) amount in common with ours, described in a patent by Martino and Paulsen (1996).",5,1,132,N,"Their approach is to build tables of the most frequent words in each language, and assign them a normalised score, based on the frequency of occurrence of the word in one language compared to the total across all the languages.",TRUE,8,TRUE,131,A further comparison,3.3,5,2,133,N,Only the most frequent words for each language are used.,5,3,134,N,"The algorithm works by accumulating scores, until a preset number of words has been read or a minimum score has been reached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W99-0206,A lot of work has been done in Japanese pronoun resolution Kameyama 1986 Yamamura et al. 1992 Walker et al. 1994 Takada and Doi 1994 Nakaiwa and Ikehara 1995 .,Nakaiwa and Ikehara 1995,129,CoCoGM,CoCoGM,UNK,0,FALSE,5,,bibr,(Kameyama 86),,,,,,,,,,,,0,-2,8,N,"For example, if the system cannot resolve zero pronouns 1, it cannot translate sentences containing them from Japanese into English.",0,-1,9,N,"When the word order of sentences is changed and the pronominalized words are changed in translation into English, the system must detect the referents of the pronouns.",0,0,10,N,A lot of work has been done in Japanese pronoun resolution (Kameyama 86) (Yamamuraet al. 92) (Walker et al. 94) (Takada & Doi 94) (Nakaiwa & Ikehara 95).,0,1,11,N,The main distinguishing features of our work are as follows:,TRUE,"0,1,2,3,4",TRUE,10,Overview,1,0,2,12,N,•,0,3,13,N,"In conventional pronoun resolution methods, semantic markers have been used for semantic constraints.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W99-0605,Knight and Graehls Knight and Graehl 1998 proposed a Japanese - English transliteration method based on the mapping probability between English and Japanese katakana sounds .,Knight and Graehl 1998,137,CoCoGM,CoCoGM,UNK,7,TRUE,3,#b14,bibr,Knight and Graehl (1998),,,,,,,,,,,,4,-2,27,N,"For problem (2), we use ""transliteration"" (Chen et al., 1998;Knight and Graehl, 1998;Wan and Verspoor, 1998).",4,-1,28,N,"Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese.",4,0,29,N,Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds.,4,1,30,N,"However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds.",TRUE,7,TRUE,29,Introduction,1,4,2,31,N,"Section 2 overviews our CLIR system, and Section 3 elaborates on the translation module focusing on compound word translation and transliteration.",4,3,32,N,Section 4 then evaluates the effectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A00-1031,The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in Ratnaparkhi 1996 .,Ratnaparkhi 1996,140,CoCoR0,CoCoR0,UNK,12,TRUE,3,#b7,bibr,"(Ratnaparkhi, 1996)",4,-2,23,N,The aim of this paper is to give a detailed account of the techniques used in TnT.,4,-1,24,N,"Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).",4,0,25,N,"The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996).",4,1,26,N,"For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).",TRUE,12,TRUE,25,Introduction,1,"According to current tagger comparisons Halteren et al. 1998 , Zavrel and Daelemans 1999 , and according to a comparsion of the results presented here with those in Ratnaparkhi 1996 , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .",Ratnaparkhi 1996,189,CoCoR0,CoCoR0,UNK,27,TRUE,170,#b7,bibr,"(Ratnaparkhi, 1996)",16,-2,180,N,They do so for several other corpora as well.,16,-1,181,N,The architecture remains applicable to a large variety of languages.,16,0,182,N,"According to current tagger comparisons (van Halteren et al., 1998;Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here.",16,1,183,N,"It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.",16,2,184,N,"TnT is freely available to universities and related organizations for research purposes (see http ://www. coli. uni-sb, de/-thorsten/tnt).",16,3,185,N,"like to thank all the people who took the effort to annotate the Penn Treebank, the Susanne Corpus, the Stuttgarter Referenzkorpus, the NEGRA Corpus, the Verbmobil Corpora, and several others.",TRUE,"25,26,27",TRUE,182,Conclusion,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A94-1009,"Similar results are presented by Merialdo 1994 , who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions .",Merialdo 1994,142,CoCoR0,CoCoR0,UNK,15,TRUE,3,#b12,bibr,Merialdo (1994),10,-2,146,N,"If neither training corpus nor lexicon are available, use BW re-estimation with standard convergence tests such as perplexity.",10,-1,147,N,"Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained.",10,0,148,N,"Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.",10,1,149,N,"As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text.",TRUE,15,TRUE,148,Discussion,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,2,150,N,"In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour.",10,3,151,N,"Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C00-1018,"On the other hand , the efficiency gains are not as big as those reported by Rayner and Carter 1996 ( but note that we cannot measure parsing times alone , so we need to compare to their speed-up factor of 10 ) .",Rayner and Carter 1996,145,CoCoR0,CoCoR0,UNK,17,TRUE,3,#b15,bibr,"Rayner and Samuelsson, 1994.",12,-2,149,N,"They induce a grammar from a treebank, while I propose to annotate the grammar based on all solutions it produces.",12,-1,150,N,"No criteria for tree decomposition and category specialization are needed here, and the standard parsing algorithm can be used.",12,0,151,N,"On the other hand, the e ciency gains are not as big as those reported by Rayner and Samuelsson, 1994.",,,,,,TRUE,17,TRUE,151,Genre Adaptation,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C00-2124,"This is different from the combination results presented in Van Halteren et al. Halteren et al. 1998 , in which pairwise voting performed best .",Halteren et al. 1998,148,CoCoR0,CoCoR0,UNK,17,TRUE,3,#b15,bibr,Halteren et al. 1998,12,-2,150,N,All combinations improve the results of the best individual classi er.,12,-1,151,N,The best results were obtained with a memory-based stacked classier.,12,0,152,N,This is di erent from the combination results presented in Van Halteren et al. 1998 Argamon et al. 1999 -91.6 91.6 91.6 Table,12,1,153,N,3: The overall performance of the majority voting combination of our best ve systems selected on tuning data performance applied to the standard data set put forward by Ramshaw and Marcus 1995 together with an overview of earlier work.,TRUE,"17,18",TRUE,152,Results 4,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12,2,154,N,"The accuracy scores indicate how often a word was classi ed correctly with the representation used O, C or IOB1.",12,3,155,N,The combined system outperforms all earlier reported results for this data set.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C96-1082,"Results using top down prediction of possible word hypotheses by the parser - work inspired by Kita et al. 1989 - have already been published in Hauenstein and Weber 1994a , Hauenstein and Weber 1994 , Weber 1994 , and Weber 1995 .",Weber 1994,158,CoCoR0,CoCoR0,UNK,8,FALSE,3,,bibr,(Kita et. al. 1989,3,-2,78,N,"This evaluation method is much harder than standard word accuracy, but it appears to be a good approximation to ""rule accuracy"".",3,-1,79,N,"Using this strict method we achieved a word accuracy of 47%, which is quite promising.",3,0,80,N,"Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a), and (Weber 1995).",3,1,81,N,Recognition rates had been improved there for read speech.,TRUE,"8,9,10",TRUE,80,Experimental Results,2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3,2,82,N,In spontaneous speech we could not achieve the same effects.,,,,,,Hauenstein and Weber 1994a,216,CoCoR0,CoCoR0,UNK,9,TRUE,203,bibr,(Weber 1994a),Weber 1995,234,CoCoR0,CoCoR0,UNK,10,TRUE,222,bibr,(Weber 1995),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P00-1061,A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in Charniak 1997 .,Charniak 1997,161,CoCoR0,CoCoR0,UNK,28,TRUE,3,#b1,bibr,Charniak (1997),20,-2,135,N,"However, the gain achieved by Beil et al. (1999) due to grammar lexicalizaton is only 2%, compared to about 10% in our case.",20,-1,136,N,"A comparison is di cult also for most other state-of-theart PCFG-based statistical parsers, since di erent training and test data, and most importantly, di erent e v aluation criteria were used.",20,0,137,N,A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in Charniak (1997).,,,,,,TRUE,28,TRUE,137,Discussion,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P88-1015,"This result challenges the claims of recent discourse theories Grosz and Sidner 1986, Reichman 1985 which argue for a the close relation between cue words and discourse structure .",Reichman 1985,162,CoCoR0,CoCoR0,UNK,0,TRUE,3,,bibr,Grosz and Sidner (5),0,-2,9,N,"In the cue words approach, Reichman'(10) has claimed that phrases like ""because"", ""so"", and ""but"" offer explicit information to listeners about how the speaker's current contribution to the discourse relates to what has gone previously.",0,-1,10,N,"For example a speaker might use the expression ""so"" to signal that s/he is about to conclude what s/he has just said.",0,0,11,N,Grosz and Sidner (5) relate the use of such phrases to changes in attentional state.,0,1,12,N,"An example would be that ""and"" or ""but"" signal to the listener that a new topic and set of referents is being introduced whereas ""anyway"" and ""in any case"" indicate a return to a previous topic and referent set.",TRUE,0,TRUE,11,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2,13,N,A second indirect way of signalling discourse structure is intonation.,0,3,14,N,Hirschberg and Pierrehumbert (7) showed that intonational contour is closely related to discourse segmentation with new topics being signalled by changes in intonational contour.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P97-1033,"Wang and Hirschberg Wang and Hirschberg 1992 did employ spontaneous speech , namely , the ATIS corpus .",Wang and Hirschberg 1992,165,CoCoR0,CoCoR0,UNK,48,TRUE,3,#b27,bibr,Wang and Hirschberg (1992),25,-2,207,N,"Their better performance is partly attributed to richer (speaker dependent) acoustic modeling, including phoneme duration, energy, and pitch.",25,-1,208,N,"However, their model was trained and tested on professionally read speech, rather than spontaneous speech.",25,0,209,N,"Wang and Hirschberg (1992) did employ spontaneous speech, namely, the ATIS corpus.",25,1,210,N,"For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents with intonational features.",TRUE,48,TRUE,209,Comparison to Other Work,8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25,2,211,N,One explanation for the difference in performance was that our model was trained on approximately ten times as much data.,25,3,212,N,"Secondly, their decision trees are used to classify each data point independently of the next, whereas we find the best interpretation over the entire turn, and incorporate speech repairs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P98-1115,Our results are similar to those reported in Krotov et al. 1994 .,Krotov et al. 1994,166,CoCoR0,CoCoR0,UNK,4,TRUE,3,#b4,bibr,"(Krotov et al., 1994)",3,-2,14,N,"We are concerned with the nature of the rule set extracted, and how it can be improved, with regard both to linguistic criteria and processing efficiency.",3,-1,15,N,"Inwhat follows, we report the worrying observation that the growth of the rule set continues at a square root rate throughout processing of the entire treebank (suggesting, perhaps that the rule set is far from complete).",3,0,16,N,"Our results are similar to those reported in (Krotov et al., 1994).",3,1,17,N,1,TRUE,4,TRUE,16,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3,2,18,N,"We discuss an alternative possible source of this rule growth phenomenon, partial bracketting, and suggest that it can be alleviated by compaction, where rules that are redundant (in a sense to be defined) are eliminated from the grammar.",3,3,19,N,Our experiments on compacting a PTB tree-1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P99-1035,Skut and Brants 1998 report 84.4% recall and 84.2% for NP and PP chunking without case labels .,Skut and Brants 1998,180,CoCoR0,CoCoR0,UNK,11,TRUE,3,#b6,bibr,"(Skut and Brants, 1998)",9,-2,116,N,"The precision measure of the first lexicalized model falls below that of the unlexicalized random model (74%), only recovering through lexicalized training to equalize the precision measure of the random model (75.6%).",9,-1,117,N,"This indicates that some degree of unlexicalized initialization is necessary, if a good lexica]ized model is to be obtained.",9,0,118,N,"(Skut and Brants, 1998) report 84.4% recall and 84.2% for NP and PP chunking without case labels.",9,1,119,N,"While these are numbers for a simpler problem and are slightly below ours, they are figures for an experiment on unrestricted sentences.",TRUE,11,TRUE,118,NC Evaluation,6.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9,2,120,N,A genuine comparison has to await extension of our model to free text.,9,3,121,N,Figure 9 gives results for verb frame recognition under the same training conditions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W97-1307,Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in ( 1 ) Lappin and Leass 1994 and ( 2 ) Kennedy and Boguraev 1996 .,Lappin and Leass 1994,184,CoCoR0,CoCoR0,UNK,17,TRUE,4,#b11,bibr,Lappin and Leass (1994),9,-2,88,N,"Table 2 shows the system's performance for pronouns broken down by two parameters, grammatical person and inter-vs. intrasentential antecedent.",9,-1,89,N,"The system did quite well (78%) with third-person pronouns with intrasentential antecedents, the largest class of such pronouns.",9,0,90,N,Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996)Kennedy and Boguraev (1996).,9,1,91,N,"For the thirdperson pronouns and reflexives, the performance was (1) 86% of 560 cases in five computer manuals and (2) 75% of 306 cases in twenty-seven Web page texts.",TRUE,"17,18,19",TRUE,90,Overall Performance,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9,2,92,N,The present FASTUS system correctly resolved 71% of 34 cases in five newspaper arti-cles.,9,3,93,N,This progressive decline in performance corresponds to the progressive decline in the amount of syntactic information in the input to reference resolution.,Kennedy and Boguraev 1996,173,CoCoR0,CoCoR0,UNK,18,FALSE,146,bibr,Kennedy and Boguraev (1996),,,,,,,,,,,#b9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W99-0605,"For further investigation , let us discuss similar experimental results reported by Kando and Aizawa Kando and Aizawa 1998 , where a bilingual dictionary produced from Japanese / English keyword pairs in the NACSIS documents is used for query translation .",Kando and Aizawa 1998,188,CoCoR0,CoCoR0,UNK,31,TRUE,4,#b12,bibr,Kando and Aizawa (1998),16,-2,154,N,"Table 4 show the recall-precision curve and l 1-point average precision for each system, respectively, from which one can see that our CLIR system is quite comparable with the monolingual IR system in performance.",16,-1,155,N,"In addition, from Figure 5 to 7, one can see that the monolingual system generally performs better at lower re(:all while the CLIR system pertbrms b(,It(,r at higher recall.",16,0,156,N,"For further investigation, let us discuss similar (~xperim(mtal results reported by Kando and Aizawa (1998), where a bilingual dictionary produced ti'om Japanese/English keyword pairs in the NACSIS documents is used for query translation.",16,1,157,N,Their evaluation method is almost the same as pertbrmed in our experinmnts.,TRUE,31,TRUE,156,Evaluation of the overall performance,4.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,2,158,N,"One difference is that they use the ""OpenText"" search engine 7, and thus the performance tbr Jal)anese-Japanese IR is higher than obtained in out"" evaluation.",16,3,159,N,"However, the performance of their Japanese-English CLIR systems, which is roughly 50-60% of that for their Japanese-Japanese IR system, is comparable with our CLIR system performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W99-0621,The results are comparable to other results reported using the Inside / Outside method Ramshaw and Marcus 1995 ( see Table  .,Ramshaw and Marcus 1995,200,CoCoR0,CoCoR0,UNK,31,TRUE,4,#b13,bibr,"(Ramshaw and Marcus, 1995",17,-1,155,N,The results of each of the predictors used in the Inside/0utside method are presented in Table 3.,17,0,156,N,"The results are comparable to other results reported using the Inside/Outside method (Ramshaw and Marcus, 1995) (see Table 7.",17,1,157,N,"We have observed that most of the mistaken predictions of base NPs involve predictions with respect to conjunctions, gerunds, adverbial NPs and some punctuation marks.",17,2,158,N,"As reported in (Argamon et al., 1998), most base NPs present in ~he data are less or equal than 4 words long.",TRUE,31,TRUE,156,Inside/Outside,5.1,This is similar to results in the literature Ramshaw and Marcus 1995 .,Ramshaw and Marcus 1995,71,CoCoR0,CoCoR0,UNK,33,TRUE,45,#b13,bibr,"(Ramshaw and Marcus, 1995)",19,-2,159,N,This implies that our predictors tend to break up long base NPs into smaller ones.,19,-1,160,N,The results also show that lexical information improves the performance by nearly 2%.,19,0,161,N,"This is similar to results in the literature (Ramshaw and Marcus, 1995).",19,1,162,N,"What we found surprising is that the second predictor, that uses additional information about the OIB status of the local context, did not do much better than the first predictor, which relies only on POS and lexical information.",19,2,163,N,A control experiment has verified that this is not due to the noisy features that the first predictor supplies to the second predictor.,19,3,164,N,"Finally, the Inside/Outside method was also tested on predicting SV phrases, yielding poor results that are not shown here.",TRUE,33,TRUE,161,Inside/Outside,5.1,17,3,159,N,This implies that our predictors tend to break up long base NPs into smaller ones.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W99-0629,"Argamon et al. 1998 report  for subject and object identification of respectively 86.5% and 83.0% , compared to 81.8% and 81.0% in this paper .",Argamon et al. 1998,235,CoCoR0,CoCoR0,UNK,41,TRUE,4,#b1,bibr,Argamon et al. (1998),17,-2,172,N,3.7% for locatives and temporals.,17,-1,173,N,Adverbial functions are more important for the two adjuncts (+6.3% resp. +15%) than for the two complements (+0.2% resp. +0.7%).,17,0,174,N,"Argamon et al. (1998) report FZ=i for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper.",17,1,175,N,"Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier.",TRUE,41,TRUE,174,Discussion,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17,2,176,N,"For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a).",17,3,177,N,"That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,

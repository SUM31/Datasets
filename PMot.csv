citation_contexts/citation_context/0/raw_citance,citation_contexts/citation_context/0/ctx_ref/0/_cit_str_original,citation_contexts/citation_context/0/ctx_ref/0/_end,citation_contexts/citation_context/0/ctx_ref/0/_function,citation_contexts/citation_context/0/ctx_ref/0/_function_originally_annotated,citation_contexts/citation_context/0/ctx_ref/0/_polarity,citation_contexts/citation_context/0/ctx_ref/0/_ref_id,citation_contexts/citation_context/0/ctx_ref/0/_reliable,citation_contexts/citation_context/0/ctx_ref/0/_start,citation_contexts/citation_context/0/ctx_ref/0/_target,citation_contexts/citation_context/0/ctx_ref/0/_type,citation_contexts/citation_context/0/ctx_ref/0/_LS,citation_contexts/citation_context/0/ctx_ref/0/__text,citation_contexts/citation_context/0/ctx_s/0/_ctx_id,citation_contexts/citation_context/0/ctx_s/0/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/0/_s_id,citation_contexts/citation_context/0/ctx_s/0/_useful,citation_contexts/citation_context/0/ctx_s/0/__text,citation_contexts/citation_context/0/ctx_s/1/_ctx_id,citation_contexts/citation_context/0/ctx_s/1/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/1/_s_id,citation_contexts/citation_context/0/ctx_s/1/_useful,citation_contexts/citation_context/0/ctx_s/1/__text,citation_contexts/citation_context/0/ctx_s/2/_ctx_id,citation_contexts/citation_context/0/ctx_s/2/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/2/_s_id,citation_contexts/citation_context/0/ctx_s/2/_useful,citation_contexts/citation_context/0/ctx_s/2/__text,citation_contexts/citation_context/0/ctx_s/3/_ctx_id,citation_contexts/citation_context/0/ctx_s/3/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/3/_s_id,citation_contexts/citation_context/0/ctx_s/3/_useful,citation_contexts/citation_context/0/ctx_s/3/__text,citation_contexts/citation_context/0/ctx_s/4/_ctx_id,citation_contexts/citation_context/0/ctx_s/4/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/4/_s_id,citation_contexts/citation_context/0/ctx_s/4/_useful,citation_contexts/citation_context/0/ctx_s/4/__text,citation_contexts/citation_context/0/_mapped,citation_contexts/citation_context/0/_ref_ids,citation_contexts/citation_context/0/_reliable,citation_contexts/citation_context/0/_s_id,citation_contexts/citation_context/0/_section_header,citation_contexts/citation_context/0/_section_number,_article_id,citation_contexts/citation_context/0/ctx_s/5/_ctx_id,citation_contexts/citation_context/0/ctx_s/5/_ctx_s_id,citation_contexts/citation_context/0/ctx_s/5/_s_id,citation_contexts/citation_context/0/ctx_s/5/_useful,citation_contexts/citation_context/0/ctx_s/5/__text,citation_contexts/citation_context/0/_hard,citation_contexts/citation_context/0/_check_ctx,citation_contexts/citation_context/0/_need_title,citation_contexts/citation_context/0/_need_abstract,citation_contexts/citation_context/1/raw_citance,citation_contexts/citation_context/1/ctx_ref/0/_cit_str_original,citation_contexts/citation_context/1/ctx_ref/0/_end,citation_contexts/citation_context/1/ctx_ref/0/_function,citation_contexts/citation_context/1/ctx_ref/0/_function_originally_annotated,citation_contexts/citation_context/1/ctx_ref/0/_polarity,citation_contexts/citation_context/1/ctx_ref/0/_ref_id,citation_contexts/citation_context/1/ctx_ref/0/_reliable,citation_contexts/citation_context/1/ctx_ref/0/_start,citation_contexts/citation_context/1/ctx_ref/0/_target,citation_contexts/citation_context/1/ctx_ref/0/_type,citation_contexts/citation_context/1/ctx_ref/0/_LS,citation_contexts/citation_context/1/ctx_ref/0/__text,citation_contexts/citation_context/1/ctx_s/0/_ctx_id,citation_contexts/citation_context/1/ctx_s/0/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/0/_s_id,citation_contexts/citation_context/1/ctx_s/0/_useful,citation_contexts/citation_context/1/ctx_s/0/__text,citation_contexts/citation_context/1/ctx_s/1/_ctx_id,citation_contexts/citation_context/1/ctx_s/1/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/1/_s_id,citation_contexts/citation_context/1/ctx_s/1/_useful,citation_contexts/citation_context/1/ctx_s/1/__text,citation_contexts/citation_context/1/ctx_s/2/_ctx_id,citation_contexts/citation_context/1/ctx_s/2/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/2/_s_id,citation_contexts/citation_context/1/ctx_s/2/_useful,citation_contexts/citation_context/1/ctx_s/2/__text,citation_contexts/citation_context/1/ctx_s/3/_ctx_id,citation_contexts/citation_context/1/ctx_s/3/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/3/_s_id,citation_contexts/citation_context/1/ctx_s/3/_useful,citation_contexts/citation_context/1/ctx_s/3/__text,citation_contexts/citation_context/1/_mapped,citation_contexts/citation_context/1/_ref_ids,citation_contexts/citation_context/1/_reliable,citation_contexts/citation_context/1/_s_id,citation_contexts/citation_context/1/_section_header,citation_contexts/citation_context/1/_section_number,citation_contexts/citation_context/0/ctx_ref/1/_cit_str_original,citation_contexts/citation_context/0/ctx_ref/1/_end,citation_contexts/citation_context/0/ctx_ref/1/_function,citation_contexts/citation_context/0/ctx_ref/1/_function_originally_annotated,citation_contexts/citation_context/0/ctx_ref/1/_polarity,citation_contexts/citation_context/0/ctx_ref/1/_ref_id,citation_contexts/citation_context/0/ctx_ref/1/_reliable,citation_contexts/citation_context/0/ctx_ref/1/_start,citation_contexts/citation_context/0/ctx_ref/1/_target,citation_contexts/citation_context/0/ctx_ref/1/_type,citation_contexts/citation_context/0/ctx_ref/1/__text,citation_contexts/citation_context/0/ctx_ref/2/_cit_str_original,citation_contexts/citation_context/0/ctx_ref/2/_end,citation_contexts/citation_context/0/ctx_ref/2/_function,citation_contexts/citation_context/0/ctx_ref/2/_function_originally_annotated,citation_contexts/citation_context/0/ctx_ref/2/_polarity,citation_contexts/citation_context/0/ctx_ref/2/_ref_id,citation_contexts/citation_context/0/ctx_ref/2/_reliable,citation_contexts/citation_context/0/ctx_ref/2/_start,citation_contexts/citation_context/0/ctx_ref/2/_target,citation_contexts/citation_context/0/ctx_ref/2/_type,citation_contexts/citation_context/0/ctx_ref/2/__text,citation_contexts/citation_context/0/_LS,citation_contexts/citation_context/1/ctx_s/4/_ctx_id,citation_contexts/citation_context/1/ctx_s/4/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/4/_s_id,citation_contexts/citation_context/1/ctx_s/4/_useful,citation_contexts/citation_context/1/ctx_s/4/__text,citation_contexts/citation_context/1/ctx_s/5/_ctx_id,citation_contexts/citation_context/1/ctx_s/5/_ctx_s_id,citation_contexts/citation_context/1/ctx_s/5/_s_id,citation_contexts/citation_context/1/ctx_s/5/_useful,citation_contexts/citation_context/1/ctx_s/5/__text,citation_contexts/citation_context/2/raw_citance,citation_contexts/citation_context/2/ctx_ref/0/_cit_str_original,citation_contexts/citation_context/2/ctx_ref/0/_end,citation_contexts/citation_context/2/ctx_ref/0/_function,citation_contexts/citation_context/2/ctx_ref/0/_function_originally_annotated,citation_contexts/citation_context/2/ctx_ref/0/_polarity,citation_contexts/citation_context/2/ctx_ref/0/_ref_id,citation_contexts/citation_context/2/ctx_ref/0/_reliable,citation_contexts/citation_context/2/ctx_ref/0/_start,citation_contexts/citation_context/2/ctx_ref/0/_target,citation_contexts/citation_context/2/ctx_ref/0/_type,citation_contexts/citation_context/2/ctx_ref/0/__text,citation_contexts/citation_context/2/ctx_s/0/_ctx_id,citation_contexts/citation_context/2/ctx_s/0/_ctx_s_id,citation_contexts/citation_context/2/ctx_s/0/_s_id,citation_contexts/citation_context/2/ctx_s/0/_useful,citation_contexts/citation_context/2/ctx_s/0/__text,citation_contexts/citation_context/2/ctx_s/1/_ctx_id,citation_contexts/citation_context/2/ctx_s/1/_ctx_s_id,citation_contexts/citation_context/2/ctx_s/1/_s_id,citation_contexts/citation_context/2/ctx_s/1/_useful,citation_contexts/citation_context/2/ctx_s/1/__text,citation_contexts/citation_context/2/ctx_s/2/_ctx_id,citation_contexts/citation_context/2/ctx_s/2/_ctx_s_id,citation_contexts/citation_context/2/ctx_s/2/_s_id,citation_contexts/citation_context/2/ctx_s/2/_useful,citation_contexts/citation_context/2/ctx_s/2/__text,citation_contexts/citation_context/2/ctx_s/3/_ctx_id,citation_contexts/citation_context/2/ctx_s/3/_ctx_s_id,citation_contexts/citation_context/2/ctx_s/3/_s_id,citation_contexts/citation_context/2/ctx_s/3/_useful,citation_contexts/citation_context/2/ctx_s/3/__text,citation_contexts/citation_context/2/ctx_s/4/_ctx_id,citation_contexts/citation_context/2/ctx_s/4/_ctx_s_id,citation_contexts/citation_context/2/ctx_s/4/_s_id,citation_contexts/citation_context/2/ctx_s/4/_useful,citation_contexts/citation_context/2/ctx_s/4/__text,citation_contexts/citation_context/2/_mapped,citation_contexts/citation_context/2/_ref_ids,citation_contexts/citation_context/2/_reliable,citation_contexts/citation_context/2/_s_id,citation_contexts/citation_context/2/_section_header,citation_contexts/citation_context/2/_section_number,citation_contexts/citation_context/3/raw_citance,citation_contexts/citation_context/3/ctx_ref/0/_cit_str_original,citation_contexts/citation_context/3/ctx_ref/0/_end,citation_contexts/citation_context/3/ctx_ref/0/_function,citation_contexts/citation_context/3/ctx_ref/0/_function_originally_annotated,citation_contexts/citation_context/3/ctx_ref/0/_polarity,citation_contexts/citation_context/3/ctx_ref/0/_ref_id,citation_contexts/citation_context/3/ctx_ref/0/_reliable,citation_contexts/citation_context/3/ctx_ref/0/_start,citation_contexts/citation_context/3/ctx_ref/0/_target,citation_contexts/citation_context/3/ctx_ref/0/_type,citation_contexts/citation_context/3/ctx_ref/0/__text,citation_contexts/citation_context/3/ctx_ref/1/_cit_str_original,citation_contexts/citation_context/3/ctx_ref/1/_end,citation_contexts/citation_context/3/ctx_ref/1/_function,citation_contexts/citation_context/3/ctx_ref/1/_function_originally_annotated,citation_contexts/citation_context/3/ctx_ref/1/_polarity,citation_contexts/citation_context/3/ctx_ref/1/_ref_id,citation_contexts/citation_context/3/ctx_ref/1/_reliable,citation_contexts/citation_context/3/ctx_ref/1/_start,citation_contexts/citation_context/3/ctx_ref/1/_target,citation_contexts/citation_context/3/ctx_ref/1/_type,citation_contexts/citation_context/3/ctx_ref/1/__text,citation_contexts/citation_context/3/ctx_ref/2/_cit_str_original,citation_contexts/citation_context/3/ctx_ref/2/_end,citation_contexts/citation_context/3/ctx_ref/2/_function,citation_contexts/citation_context/3/ctx_ref/2/_function_originally_annotated,citation_contexts/citation_context/3/ctx_ref/2/_polarity,citation_contexts/citation_context/3/ctx_ref/2/_ref_id,citation_contexts/citation_context/3/ctx_ref/2/_reliable,citation_contexts/citation_context/3/ctx_ref/2/_start,citation_contexts/citation_context/3/ctx_ref/2/_target,citation_contexts/citation_context/3/ctx_ref/2/_type,citation_contexts/citation_context/3/ctx_ref/2/__text,citation_contexts/citation_context/3/ctx_s/0/_ctx_id,citation_contexts/citation_context/3/ctx_s/0/_ctx_s_id,citation_contexts/citation_context/3/ctx_s/0/_s_id,citation_contexts/citation_context/3/ctx_s/0/_useful,citation_contexts/citation_context/3/ctx_s/0/__text,citation_contexts/citation_context/3/ctx_s/1/_ctx_id,citation_contexts/citation_context/3/ctx_s/1/_ctx_s_id,citation_contexts/citation_context/3/ctx_s/1/_s_id,citation_contexts/citation_context/3/ctx_s/1/_useful,citation_contexts/citation_context/3/ctx_s/1/__text,citation_contexts/citation_context/3/ctx_s/2/_ctx_id,citation_contexts/citation_context/3/ctx_s/2/_ctx_s_id,citation_contexts/citation_context/3/ctx_s/2/_s_id,citation_contexts/citation_context/3/ctx_s/2/_useful,citation_contexts/citation_context/3/ctx_s/2/__text,citation_contexts/citation_context/3/ctx_s/3/_ctx_id,citation_contexts/citation_context/3/ctx_s/3/_ctx_s_id,citation_contexts/citation_context/3/ctx_s/3/_s_id,citation_contexts/citation_context/3/ctx_s/3/_useful,citation_contexts/citation_context/3/ctx_s/3/__text,citation_contexts/citation_context/3/ctx_s/4/_ctx_id,citation_contexts/citation_context/3/ctx_s/4/_ctx_s_id,citation_contexts/citation_context/3/ctx_s/4/_s_id,citation_contexts/citation_context/3/ctx_s/4/_useful,citation_contexts/citation_context/3/ctx_s/4/__text,citation_contexts/citation_context/3/ctx_s/5/_ctx_id,citation_contexts/citation_context/3/ctx_s/5/_ctx_s_id,citation_contexts/citation_context/3/ctx_s/5/_s_id,citation_contexts/citation_context/3/ctx_s/5/_useful,citation_contexts/citation_context/3/ctx_s/5/__text,citation_contexts/citation_context/3/_mapped,citation_contexts/citation_context/3/_ref_ids,citation_contexts/citation_context/3/_reliable,citation_contexts/citation_context/3/_s_id,citation_contexts/citation_context/3/_section_header,citation_contexts/citation_context/3/_section_number,citation_contexts/citation_context/1/ctx_ref/1/_cit_str_original,citation_contexts/citation_context/1/ctx_ref/1/_end,citation_contexts/citation_context/1/ctx_ref/1/_function,citation_contexts/citation_context/1/ctx_ref/1/_function_originally_annotated,citation_contexts/citation_context/1/ctx_ref/1/_polarity,citation_contexts/citation_context/1/ctx_ref/1/_ref_id,citation_contexts/citation_context/1/ctx_ref/1/_reliable,citation_contexts/citation_context/1/ctx_ref/1/_start,citation_contexts/citation_context/1/ctx_ref/1/_target,citation_contexts/citation_context/1/ctx_ref/1/_type,citation_contexts/citation_context/1/ctx_ref/1/__text,citation_contexts/citation_context/1/ctx_ref/2/_cit_str_original,citation_contexts/citation_context/1/ctx_ref/2/_end,citation_contexts/citation_context/1/ctx_ref/2/_function,citation_contexts/citation_context/1/ctx_ref/2/_function_originally_annotated,citation_contexts/citation_context/1/ctx_ref/2/_polarity,citation_contexts/citation_context/1/ctx_ref/2/_ref_id,citation_contexts/citation_context/1/ctx_ref/2/_reliable,citation_contexts/citation_context/1/ctx_ref/2/_start,citation_contexts/citation_context/1/ctx_ref/2/_target,citation_contexts/citation_context/1/ctx_ref/2/_type,citation_contexts/citation_context/1/ctx_ref/2/__text,citation_contexts/citation_context/1/ctx_ref/3/_cit_str_original,citation_contexts/citation_context/1/ctx_ref/3/_end,citation_contexts/citation_context/1/ctx_ref/3/_function,citation_contexts/citation_context/1/ctx_ref/3/_function_originally_annotated,citation_contexts/citation_context/1/ctx_ref/3/_polarity,citation_contexts/citation_context/1/ctx_ref/3/_ref_id,citation_contexts/citation_context/1/ctx_ref/3/_reliable,citation_contexts/citation_context/1/ctx_ref/3/_start,citation_contexts/citation_context/1/ctx_ref/3/_target,citation_contexts/citation_context/1/ctx_ref/3/_type,citation_contexts/citation_context/1/ctx_ref/3/__text,citation_contexts/citation_context/1/_LS,citation_contexts/citation_context/0/ctx_s/0/_LS,citation_contexts/citation_context/0/_need_fulltext,citation_contexts/citation_context/1/_need_title,citation_contexts/citation_context/1/_need_asbtract,citation_contexts/citation_context/0/_in_collate,citation_contexts/citation_context/0/_check_again,citation_contexts/citation_context/0/ctx_ref/3/_cit_str_original,citation_contexts/citation_context/0/ctx_ref/3/_end,citation_contexts/citation_context/0/ctx_ref/3/_function,citation_contexts/citation_context/0/ctx_ref/3/_function_originally_annotated,citation_contexts/citation_context/0/ctx_ref/3/_polarity,citation_contexts/citation_context/0/ctx_ref/3/_ref_id,citation_contexts/citation_context/0/ctx_ref/3/_reliable,citation_contexts/citation_context/0/ctx_ref/3/_start,citation_contexts/citation_context/0/ctx_ref/3/_target,citation_contexts/citation_context/0/ctx_ref/3/_type,citation_contexts/citation_context/0/ctx_ref/3/__text,citation_contexts/citation_context/0/_broken_by_footnote,citation_contexts/citation_context/0/_broken,citation_contexts/citation_context/1/_need_abstract,citation_contexts/citation_context/1/ctx_s/0/_LS
"As evidence of the importance of this task in the NLP community note that the early , influential system SHRDLU Winograd 1973 was intended to address just this type of problem .",Winograd 1973,127,PMot,PMot,UNK,0,TRUE,111,#b16,bibr,ref motivates task of this paper,"(Winograd, 1973)",0,-1,3,Y,The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.,0,0,4,Y,"As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem.",0,1,5,N,"More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998;Perzanowski et al., 1999).",0,2,6,N,A number of other systems have addressed part of the task.,0,3,7,N,"Com-mandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and   (Traum and Allen, 1994;Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents.",TRUE,0,TRUE,4,Introduction,1,A00-1016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nevertheless , a recent independent comparison of 7 taggers Zavrel and Daelemans 1999 has shown that another approach even works better : Markov models combined with a good smoothing technique and with handling of unknown words .",Zavrel and Daelemans 1999,87,PMot,PMot,UNK,7,TRUE,59,#b13,bibr,refer to title; the paper is interesting. The main purpose seems to confirnm the usefulness of an algorithm proposed by someone else.,"(Zavrel and Daelemans, 1999)",1,-2,11,N,"They are only surpassed by combinations of different systems, forming a ""voting tagger"".",1,-1,12,N,"Among the statistical approaches, the Maximum Entropy framework has a very strong position.",1,0,13,Y,"Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better:",1,1,14,Y,Markov models combined with a good smoothing technique and with handling of unknown words.,1,2,15,Y,"This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging.",TRUE,7,TRUE,13,Introduction,1,A00-1031,1,3,16,N,"The tagger comparison was organized as a ""blackbox test"": set the same task to every tagger and compare the outcomes.",TRUE,YES,TRUE,TRUE,"Currently , the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in Samuelsson 1993 .",Samuelsson 1993,150,PMot,PMot,UNK,16,TRUE,132,#b8,bibr,"From the section header, seems to be Method section","(Samuelsson, 1993)",7,0,64,Y,"Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993).",7,1,65,N,Tag probabilities are set according to the word's ending.,7,2,66,N,"The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (JJ) in 98% of the cases (e.g. fashionable, variable), the rest of 2% are nouns (e.g. cable, variable).",7,3,67,N,The probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum length.,TRUE,16,TRUE,64,Handling of Unknown Words,2.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Henderson and Brill 1999 showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy .,Henderson and Brill 1999,166,PMot,PMot,UNK,6,TRUE,140,#b8,bibr,,Henderson and Brill (1999),3,-2,39,N,Create parser f~ = g(Ci) for each i.,3,-1,40,N,3,3,0,41,N,"Given a novel sentence 8test E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999).",,,,,,,,,,,TRUE,6,FALSE,41,Algorithm: Bagging A Parser (2),UNKNOW SECTION NUMBER,A00-2005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Despite making such an assumption , this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies ( e.g. , Leacock et al. 1993 , Mooney 1996 , Ng and Lee 1996 , Pedersen and Bruce 1997 ) .",Pedersen and Bruce 1997,196,PMot,PMot,UNK,1,FALSE,174,#b7,bibr,,"(Leacock et al., 1993)",1,-2,6,N,This is motivated by the observation that enhancing the feature set or learning algorithm used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm.,1,-1,7,Y,"For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model.",1,0,8,Y,"Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996)(Ng and Lee, 1996), (Pealersen and ).",1,1,9,N,These studies represent the context in which an ambiguous word occurs with a wide variety of features.,1,2,10,N,"However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.",TRUE,"1,2,3,4",TRUE,8,Introduction,1,A00-2009,1,3,11,N,"It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)).",,,TRUE,TRUE,"It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble ( e.g. , Dietterich 1997 ) .",Dietterich 1997,218,PMot,PMot,UNK,5,TRUE,200,#b2,bibr,Ensemble in title,"(Dietterich, 1997)",2,-2,9,N,These studies represent the context in which an ambiguous word occurs with a wide variety of features.,2,-1,10,N,"However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.",2,0,11,Y,"It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)).",2,1,12,N,"In natural language processing, ensemble techniques have been successfully applied to partof-speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)).",TRUE,5,TRUE,11,Introduction,1,Mooney 1996,212,PMot,PMot,UNK,2,TRUE,198,#b9,bibr,"(Mooney, 1996)",Ng and Lee 1996,232,PMot,PMot,UNK,3,FALSE,214,#b10,bibr,"(Ng and Lee, 1996)",naive bayesian classfier (-1) also in title,2,2,13,N,"When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble.",2,3,14,N,This paper begins with an introduction to the Naive Bayesian classifier.,"A recent survey of such results , as well as possible explanations for its success , is presented in Domingos and Pazzani 1997 .",Domingos and Pazzani 1997,127,PMot,PMot,UNK,26,TRUE,99,#b3,bibr,"(Domingos and Pazzani, 1997)",17,-1,120,Y,The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies.,17,0,121,Y,"A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997).",17,1,122,N,"A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), ).",17,2,123,N,In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data.,17,3,124,N,"For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data.",TRUE,26,TRUE,121,Naive Bayesian classifiers,5.1,"A similar finding has emerged in word sense disambiguation , where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier ( e.g. , Leacock et al. 1993 , Mooney 1996 , Ng and Lee 1996 , Pedersen and Bruce 1997 ) .",Pedersen and Bruce 1997,235,PMot,PMot,UNK,27,FALSE,213,#b7,bibr,"(Leacock et al., 1993)",Mooney 1996,251,PMot,PMot,UNK,28,TRUE,237,#b9,bibr,"(Mooney, 1996)",Ng and Lee 1996,271,PMot,PMot,UNK,29,TRUE,253,#b10,bibr,"(Ng and Lee, 1996)",18,-2,120,Y,The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies.,18,-1,121,Y,"A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997).",18,0,122,Y,"A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), ).",18,1,123,N,In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data.,18,2,124,N,"For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data.",18,3,125,N,"This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different.",TRUE,"27,28,29",TRUE,122,Naive Bayesian classifiers,5.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The most influential problem in motivating statistical learning application in NLP tasks is that of word selection in speech recognition Jelinek 1998 .,Jelinek 1998,152,PMot,PMot,UNK,1,TRUE,137,#b8,bibr,current task is word prediction (title),"(Jelinek, 1998)",1,0,18,Y,"The most influential problem in motivating statistical learning application in NLP tasks is that of word selection in speech recognition (Jelinek, 1998).",1,1,19,N,"There, word classifiers are derived from a probabilistic language model which estimates the probability of a sentence s using Bayes rule as the product of conditional probabilities,",,,,,,,,,,,,,,,,TRUE,1,TRUE,18,Background,UNKNOW SECTION NUMBER,A00-2017,,,,,,,,,,"Earlier versions of SNoW Roth 1998 , Golding and Roth 1999 , Roth and Zelenko 1998 , Munoz et al. 1999 have been applied successfully to several natural language related tasks .",Roth 1998,37,PMot,PMot,UNK,35,TRUE,25,#b17,bibr,,"(Roth, 1998;",19,-2,115,N,The Learning Approach,19,-1,116,Y,"Our experimental investigation is done using the SNo W learning system (Roth, 1998).",19,0,117,Y,"Earlier versions of SNoW (Roth, 1998;Golding and Roth, 1999;Roth and Zelenko, 1998;Munoz et al., 1999) have been applied successfully to several natural language related tasks.",19,1,118,N,Here we use SNo,TRUE,"35,36,37,38",TRUE,117,3,UNKNOW SECTION NUMBER,,,,,,,,,,,,,,,,,,,,,,,,19,2,119,N,"W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Golding and Roth 1999,60,PMot,PMot,UNK,36,TRUE,37,#b6,bibr,"Golding and Roth, 1999;",Roth and Zelenko 1998,83,PMot,PMot,UNK,37,TRUE,60,#b16,bibr,"Roth and Zelenko, 1998;",Munoz et al. 1999,102,PMot,PMot,UNK,38,TRUE,83,#b13,bibr,"Munoz et al., 1999)",motivate the use of tool,,,,,,,,,,,,,,,,,,,,,
One of the most effective taggers based on a pure HMM is that developed at Xerox Cutting et al. 1992 .,Cutting et al. 1992,103,PMot,PMot,UNK,9,TRUE,81,#b3,bibr,"link back to asbtract, like HMM and tagger","(Cutting et al., 1992)",4,-2,24,N,"The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s.",4,-1,25,N,"With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992;Brill, 1992), DeRose (DeRose, 1988) and gupiec (Kupiec, 1992).",4,0,26,Y,"One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992).",4,1,27,Y,An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.,4,2,28,N,"96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus.",TRUE,9,TRUE,26,Background,1,A94-1009,4,3,29,N,The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible.,,,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Top - down parsing techniques are attractive because of their simplicity , and can often achieve good performance in practice Roark and Johnson 1999 .",Roark and Johnson 1999,146,PMot,PMot,UNK,0,TRUE,123,#b15,bibr,,"Roark and Johnson, 1999",0,0,4,Y,"Top-down parsing techniques are attractive because of their simplicity, and can often achieve good performance in practice Roark and Johnson, 1999.",0,1,5,N,"However, with a left-recursive grammar such parsers typically fail to terminate.",0,2,6,N,"The left-corner grammar transform converts a left-recursive grammar into a non-left-recursive one: a top-down parser using a left-corner transformed grammar simulates a left-corner parser using the original grammar Rosenkrantz and Lewis II, 1970;Aho and Ullman, 1972.",0,3,7,N,"However, the left-corner transformed grammar can be signi cantly larger than the original grammar, causing numerous problems.",,,,,,TRUE,0,TRUE,4,Introduction,UNKNOW SECTION NUMBER,C00-1052,,,,,,,,TRUE,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,link back to abstract,,,,,,,,,,,,,,,,,,,,
Van Halteren et al. Halteren et al. 1998 and Brill and Wu Brill and Wu 1998 show that part-of-speech tagger performance can be improved by combining different taggers .,Brill and Wu 1998,44,PMot,PMot,UNK,2,FALSE,37,#b3,bibr,,Wu 1998,0,0,3,Y,Van Halteren et al. 1998 andBrill andWu 1998Halteren et al. 1998 andBrill andWu 1998 show that part-of-speech tagger performance can be improved by combining di erent taggers.,0,1,4,N,"By using techniques such as majority voting, errors made by the minority of the taggers can be removed.",0,2,5,N,Van Halteren et al. 1998 report that the results of such a combined approach can improve upon the accuracy error of the best individual system with as much as 19.,0,3,6,N,The positive e ect of system combination for non-language processing tasks has been shown in a large body of machine learning work.,,,,,,TRUE,"0,1,2,3",TRUE,3,Introduction,UNKNOW SECTION NUMBER,C00-2124,,,,,,,,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Halteren et al. 1998,24,PMot,PMot,UNK,3,TRUE,4,#b15,bibr,Halteren et al. 1998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Japanese nouns have no equivalent to the English singular and plural forms and verbs do not inflect to agree with the number of the subject Kuno 1973 .,Kuno 1973,151,PMot,PMot,UNK,0,TRUE,140,#b4,bibr,difficult problem (-2) -- explanation in (0): motivate task,(Kuno 1973),0,-2,3,Y,Correctly determining number is a difficult problem when translating from Japanese to English.,0,-1,4,Y,"This is because in Japanese, noun phrases are not normally marked with respect to number.",0,0,5,Y,Japanese nouns have no equivalent to the English singular and plural forms and verbs do not inflect to agree with the number of the subject (Kuno 1973).,0,1,6,N,"In addition, there is no grammatical marking of countability, t",0,2,7,N,"In order to generate English correctly, it is necessary to know whether a given noun phrase is countable or uncountable and, if countable, whether it is singular or plural.",TRUE,0,TRUE,5,Introduction,1,C94-1002,0,3,8,N,"Deciding this is a problem even for humans translating from Japanese to English, but they have their own knowledge of both languages t Japanese does not have obligatory plural morphemes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Also , the use of collocations in different applications has been discussed by various authors McRoy 1992 , Pustejovsky et al. 1992 , Smadja and McKeown 1990 .",McRoy 1992,108,Neut,PMot,UNK,3,TRUE,94,#b6,bibr,no positive sentiment detected,"((McRoy, 1992)",1,-2,14,N,"Financial contributions weir by the Commission of the European Community, Association Suissetra (Geneva) and Oxford University Press.",1,-1,15,N,"word sense disambiguation, information retrieval, natural language generation and so on.",1,0,16,Y,"Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992), (Pnstejovsky et al., 1992), (Smadja and McKeown, 1990) etc.).",1,1,17,N,"However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988), machine translation, (Heid and Raab, 1989)) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989), (Krenn and Erbach, to appear)).",1,2,18,N,We have been concerned with investigating the lexical .,TRUE,"3,4,5",TRUE,16,Description of the Problem,1,C94-2202,1,3,19,N,"['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates.",TRUE,,,TRUE,Our work is related to research by Heid and Raab 1989 .,Nirenburg et al. 1988,144,PMot,PMot,UNK,6,TRUE,120,#b8,bibr,,"(Nirenburg et al., 1988)",2,-2,15,N,"word sense disambiguation, information retrieval, natural language generation and so on.",2,-1,16,N,"Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992), (Pnstejovsky et al., 1992), (Smadja and McKeown, 1990) etc.).",2,0,17,Y,"However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988), machine translation, (Heid and Raab, 1989)) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989), (Krenn and Erbach, to appear)).",2,1,18,N,We have been concerned with investigating the lexical .,TRUE,"6,7,8,9",TRUE,17,Description of the Problem,1,Pustejovsky et al. 1992,136,Neut,PMot,UNK,4,TRUE,110,,bibr,"(Pnstejovsky et al., 1992)",Smadja and McKeown 1990,164,Neut,PMot,UNK,5,TRUE,138,#b10,bibr,"(Smadja and McKeown, 1990)",,2,2,19,N,"['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates.",2,3,20,N,"Our work is related to research by (Heid and Raab, /989).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Abeille and Schabes 1989,265,PMot,PMot,UNK,8,FALSE,232,,bibr,"(e.g. (Abeill6 and Schabes, 1989)",Krenn and Erbach To appear,296,PMot,PMot,UNK,9,TRUE,267,,bibr,"(Krenn and Erbach, to appear)",,,,,,,,,,,,"motivate problem, link by machine translation",,TRUE,TRUE,TRUE,,,,,,,,,,,,,,,,,
"Machine Readable Dictionaries ( MRDs ) are a good source of lexical information and have been shown to be applicable to the task of LKB construction Dolan et al. 1993 , Calzolari 1992 , Copestake 1990 , Wilks et al. 1989 , Byrd et al. 1987 .",Copestake 1990,176,PMot,PMot,UNK,0,FALSE,166,#b10,bibr,,"al., 1993;",0,-1,5,Y,"When constructing a l,exieal Knowledge Ilase (1,KB) useful for Natural l,anguage Processing, the source of information from which knowledge is acquired and the structuring of this information within the LKB are two key issues.",0,0,6,Y,"Machine Readable Dictionaries (MIH)s) are a good sour(:e of lexical information and have been shown to be al)plical)le to the task of I,KII COllStruction (l)ola.n ct al., 1993;Calzolari, t992;Copestake, [990;Wilks et al., 1989;Byrd et al., 1987).",0,1,7,N,"Often though, a localist approaeh is adopted whereby the words are kept in alphabetical order with some representation of their definitions in the form of a template or feature structure.",0,2,8,N,"F, flbrt in findlug cormections between words is seen in work on automatic extraction of sem~mtic relations Dora MRI)s (Ahlswede and Evens, 1988;Alshawi, 1989;Montemagrfi and Vandorwende, 19!32).",0,3,9,N,"Additionally, effort in finding words that are close semantically is seen by the current interest in statistical techniques for word clustering, looking at (-ooccurrences of words in text corpora or dictionaries (Church and IIanks, 1989;Wilks et al., 1989;Brown et al., 11992;l'ereira et al., 11995).",TRUE,"0,1,2,3,4",TRUE,6,it Introduction,UNKNOW SECTION NUMBER,C96-1013,,,,,,,,TRUE,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wilks et al. 1989,227,PMot,PMot,UNK,3,TRUE,208,#b18,bibr,"Wilks et al., 1989;",Byrd et al. 1987,245,PMot,PMot,UNK,4,TRUE,227,#b5,bibr,"Byrd et al., 1987)",link back to title and abstract,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TRUE,TRUE,,,,,,,,,,,,,,,
One of the valuable indicators of the structure of text is lexical cohesion Halliday and Hasan 1976 .,Halliday and Hasan 1976,26,PMot,PMot,UNK,1,TRUE,0,#b7,bibr,lexical cohesion in abstract: two paragraphs later: we consider lexical cohesion as similarity,"[Halliday and Hasan, 1976]",1,-2,7,N,"[Grosz and Sidner, 1986]",1,-1,8,Y,One of the valuable indicators of the structure of text is lexical cohesion.,1,0,9,Y,"[Halliday and Hasan, 1976] Lexical cohesion is the relationship between words, classified as follows:",,,,,,,,,,,TRUE,1,TRUE,9,Introduction,1,E93-1028,,,,,,TRUE,,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Accounts of various linguistic phenomena have been developed within the framework on which our extension is based , including quantifiers and anaphora Dalrymple et al. 1994a , intensional verbs Dalrymple et al. 1994b , and complex predicates Dalrymple et al. 1993a .",Dalrymple et al. 1993a,175,PMot,PMot,UNK,15,FALSE,150,#b2,bibr,,"(Dalrymple et al., 1994a)",14,-2,106,N,"A second derivation is also possible, in which supported and NAFTA are combined first and the result is then combined with Bill.",14,-1,107,Y,The use of linear logic provides a flexible mechanism for deducing meanings of sentences based on their f-structure representations.,14,0,108,Y,"Accounts of various linguistic phenomena have been developed within the framework on which our extension is based, including quantifiers and anaphora (Dalrymple et al., 1994a), intensional verbs (Dalrympie et al., 1994b)(Dalrymple et al., 1994a), intensional verbs (Dalrympie et al., 1994b), and complex predicates (Dalrymple et al.,!993b).",14,1,109,N,The logic fits well with the 'resource-sensitivity' of natural language semantics: there is a one-to-one correspondence between f-structure relationships and meanings; the multiple use of resources arises from multiple paths to them in the f-structure.,14,2,110,N,"In the next section, we show how this system applies to several cases of right-node raising.",TRUE,"15,16,17,18",TRUE,108,"(15) (f suBJ)a""~Bill",UNKNOW SECTION NUMBER,E95-1005,,,,,,,,,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dalrymple et al. 1994b,220,PMot,PMot,UNK,16,TRUE,195,,bibr,"(Dalrympie et al., 1994b)",,,,,,,,,,,,not PBas: we based on the framework; the framework is flexible (-1) and the accounts of various linguistic phenomena been developed within the (lilear logic) framework; f-structure and meaning (-1) linked back to abstract,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Part-of-speech tagging is of interest for a number of applications , for example access to text data bases Kupiec 1993 , robust parsing Abney 1991 , and general parsing de Marcken 1990, Charniak et al. 1994 .",Kupiec 1993,120,PMot,PMot,UNK,0,TRUE,106,#b20,bibr,,"(Kupiec, 1993)",0,-2,4,N,"This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words.",0,-1,5,Y,"In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging.",0,0,6,Y,"Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990;Charniak et al., 1994).",0,1,7,N,"The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages.",0,2,8,N,"While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest.",TRUE,"0,1,2,3",TRUE,6,Introduction,1,E95-1020,0,3,9,N,"The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Abney 1991,150,PMot,PMot,UNK,1,TRUE,137,#b0,bibr,"(Abney, 1991)",de Marcken 1990,189,PMot,PMot,UNK,2,TRUE,172,#b12,bibr,"(deMarcken, 1990;",necessity of research in part-of-speech tagging: motivate research problem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Charniak et al. 1994,211,PMot,PMot,UNK,3,TRUE,189,#b8,bibr,"Charniak et al., 1994)",,,,
This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use Appelt 1987 .,Appelt 1987,131,PMot,PMot,UNK,1,TRUE,118,#b0,bibr,,"Appelt, 1987)",1,-1,3,Y,"Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a).",1,0,4,Y,"This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987).",1,1,5,Y,"We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor.",1,2,6,N,The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars.,1,3,7,N,"In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation.",TRUE,1,TRUE,4,Introduction,1,E95-1024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,research topic (-1) -- advantage (0) -- address this topic (+1),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Experimental results confirming this wisdom have been presented , e.g. , by Elworthy 1994 and Pereira and Schabes 1992 for EM training of Hidden Markov Models and PCFGs .",Elworthy 1994,87,PMot,PMot,UNK,5,TRUE,72,#b6,bibr,,Elworthy (1994),3,-2,10,N,"In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which o b viates the standard e ort required for treebank training handannotating large corpora of speci c domains of speci c languages with speci c parse types.",3,-1,11,N,"Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied.",3,0,12,Y,"Experimental results con rming this wisdom have beenpresented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs.",3,1,13,Y,"In this paper, we present a new lexicalized stochastic model for constraint-based grammars that employs a combination of headword frequencies and EM-based clustering for grammar lexicalization.",3,2,14,N,"Furthermore, we make crucial use of EM for estimating the parameters of the stochastic grammar from unannotated data.",TRUE,"5,6",TRUE,12,Introduction,UNKNOW SECTION NUMBER,P00-1061,3,3,15,N,Our usage of EM was initiated by the current lack of large uni cationbased treebanks for German.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pereira and Schabes 1992,118,PMot,PMot,UNK,6,TRUE,92,#b15,bibr,Pereira and Schabes (1992),,,,,,,,,,,,ref for EM (0) -- we present ... that employs ... EM-based (+1),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Magic ( templates ) is a general compilation technique for efficient bottom-up evaluation of logic programs developed in the deductive database community Ramakrishnan et al. 1992 .,Ramakrishnan et al. 1992,132,PMot,PMot,UNK,0,TRUE,105,#b8,bibr,,"(Ramakrishnan et al., 1992)",0,0,7,N,"nique for efficient bottom-up evaluation of logic programs developed in the deductive database community (Ramakrishnan et al., 1992).",0,1,8,N,"Given a logic program, Magic produces a new program in which the filtering as normally resulting from top-down evaluation is explicitly characterized through, so-called, *url: http://www.sfs.nphil.uni-tuebingen/'minnen magic predicates, which produce variable bindings for filtering when evaluated bottom-up.",0,2,9,N,The original rules of the program are extended such that these bindings can be made effective.,0,3,10,N,"As a result of the definite clause characterization of filtering, Magic brings filtering into the logic underlying the grammar.",,,,,,TRUE,0,TRUE,7,Magic (templates) is a general compilation tech-,UNKNOW SECTION NUMBER,P96-1033,,,,,,,,TRUE,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TRUE,,
A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems Carroll 1994 .,Carroll 1994,154,PMot,PMot,UNK,0,TRUE,139,#b0,bibr,positive criticism motivates (further) study of the research problem,"(Carroll, 1994)",0,0,4,Y,"A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994).",0,1,5,N,"In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991).",0,2,6,N,Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time.,0,3,7,N,"However, in the context of unification-based parsing, left-recursive grammars have the formal power of a Turing machine, therefore detection of all infinite loops due to left-recursion is impossible (Shieber, 1992).",,,,,,TRUE,0,TRUE,4,Introduction,1,P96-1058,,,,,,,,TRUE,TRUE,"In particular , top-down filtering seems to be very effective in increasing parse efficiency Shann 1991 .",Shann 1991,105,PMot,PMot,UNK,1,TRUE,92,#b4,bibr,positive criticism motivates (further) study of the research problem,"(Shann, 1991)",1,-1,4,Y,"A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994).",1,0,5,Y,"In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991).",1,1,6,N,Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time.,1,2,7,N,"However, in the context of unification-based parsing, left-recursive grammars have the formal power of a Turing machine, therefore detection of all infinite loops due to left-recursion is impossible (Shieber, 1992).",TRUE,1,TRUE,5,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,1,3,8,N,"So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TRUE,,,,,,,,,,,,,,,,,TRUE,
"These unseen events generally make up a substantial portion of novel data ; for example , Essen and Steinbiss 1992 report that 12% of the test-set bigrams in a 75% - 25% split of one million words did not occur in the training partition .",Essen and Steinbiss 1992,114,PMot,PMot,UNK,0,TRUE,88,#b6,bibr,gap/weak motivates (further) study of the research problem,Essen and Steinbiss (1992),0,-2,2,N,An inherent problem for statistical methods in natural language processing is that of sparse data --the inaccurate representation in any training corpus of the probability of low frequency events.,0,-1,3,N,"In particular, reasonable events that happen to not occur in the training set may mistakenly be assigned a probability of zero.",0,0,4,Y,"These unseen events generally make up a substantial portion of novel data; for example, Essen and Steinbiss (1992) report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition.",0,1,5,Y,"We consider here the question of how to estimate the conditional cooccurrence probability P(v[n) of an unseen word pair (n, v) drawn from some finite set N x V.",0,2,6,N,Two state-of-the-art technologies are Katz's (1987) backoff method and Jelinek and Mercer's (1980) interpolation method.,TRUE,0,TRUE,4,Introduction,1,P99-1004,0,3,7,N,"Both use P(v) to estimate P(v[n) when (n, v) is unseen, essentially ignoring the identity of n.",,,,,"In previous work Dagan et al. 1999 , we compared the performance of three different functions : the Jensen - Shannon divergence ( total divergence to the average ) , the L1 norm , and the confusion probability .",Dagan et al. 1999,37,PMot,PMot,UNK,6,TRUE,17,#b5,bibr,,"(Dagan et al., 1999)",5,-2,11,Y,"We focus on distributional rather than semantic similarity (e.g., Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions --although the words ""chance"" and ""probability"" are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in.",5,-1,12,Y,There are many plausible measures of distributional similarity.,5,0,13,Y,"In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.",5,1,14,N,Our experiments on a frequency-controlled pseudoword disambiguation task showed that using any of the three in a distance-weighted averaging scheme yielded large improvements over Katz's backoff smoothing method in predicting unseen coocurrences.,TRUE,6,TRUE,13,Introduction,1,,,,,,,,,,,,,,,,,,,,,,,,5,2,15,N,"Furthermore, by using a restricted version of model (1) that stripped incomparable parameters, we were able to empirically demonstrate that the confusion probability is fundamentally worse at selecting useful similar words.",5,3,16,N,D. Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,positive criticism motivates focus of the research problem
"Charniak 1995 and Carroll and Rooth 1998 present head-lexicalized probabilistic context free grammar formalisms , and show that they can effectively be applied in inside-outside estimation of syntactic language models for English , the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates .",Charniak 1995,30,PMot,PMot,UNK,0,TRUE,15,#b1,bibr,,Charniak (1995),0,0,3,Y,"1 Introduction Charniak (1995) and Carroll and Rooth (1998) present head-lexicalized probabilistic context free grammar formalisms, and show that they can effectively be applied in inside-outside estimation of syntactic language models for English, the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates.",0,1,4,Y,"The present paper describes an experiment where a slightly modified version of Carroll and Rooth's model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order).",0,2,5,N,We emphasize techniques which made it practical to apply inside-outside estimation of a lexicalized context free grammar to such a language.,0,3,6,N,"These techniques relate to the treatment of argument cancellation and scrambled phrase order; to the treatment of case features in category labels; to the category vocabulary for nouns, articles, adjectives and their projections; to lexicalization based on uninflected lemmata rather than word forms; and to exploitation of a parameter-tying feature.",,,,,,TRUE,"0,1",TRUE,3,1 Introduction Charniak (1995) and Carroll and Roo,UNKNOW SECTION NUMBER,P99-1035,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Carroll and Rooth 1998,59,PMot,PMot,UNK,1,TRUE,35,#b9,bibr,Carroll and Rooth (1998),,,,,,,,,,,,used in this paper (+1) motivated by the positive criticism (0),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"However , Pirkola Pirkola 1998 , for example , used a subset of the TREC collection related to health topics , and showed that combination of general and domain specific ( i.e. , medical ) dictionaries improves the CLIR performance obtained with only a general dictionary .",Pirkola 1998,23,PMot,PMot,UNK,0,TRUE,9,#b20,bibr,,Pirkola (1998),0,-2,17,N,"As with MT systems, existing CLIR systems still find it difficult to translate technical terms and proper nouns, which are often unlisted in general dictionaries.",0,-1,18,N,"Since most CLIR systems target newspaper articles, which are comprised mainly of general words, the problem related to unlisted words has been less explored than other CLIR subtopics (such as resolution of translation ambiguity).",0,0,19,Y,"However, Pirkola (1998), for example, used a subset of the TREC collection related to health topics, and showed that combination of general and domain specific (i.e., medical) dictionaries improves the CLIR performance obtained with only a general dictionary.",0,1,20,Y,This result shows the potential contribution of technical term translation to CLIR.,0,2,21,N,"At the same time, note that even domain specific dictionaries lhttp ://www. rd. nacs is.",TRUE,0,TRUE,19,Introduction,1,W99-0605,0,3,22,N,"ac. j p/-nt cadm/index-en, html do not exhaustively list possible technical terms.",,,TRUE,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Recently , combination techniques have been investigated for part of speech tagging with positive results van Halteren et al. 1998 , Brill and Wu 1998 .",van Halteren et al. 1998,132,PMot,PMot,UNK,2,TRUE,105,#b8,bibr,,"(van Halteren et al., 1998;",1,-2,5,N,"Their theoretical I finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers.",1,-1,6,N,The theory has also been validated empirically.,1,0,7,Y,"Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998;Brill and Wu, 1998).",1,1,8,N,In both cases the investigators were able to achieve significant improvements over the previous best tagging results.,1,2,9,N,"Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).",TRUE,"2,3",TRUE,7,Introduction,1,W99-0623,1,3,10,N,"The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997).",,,TRUE,TRUE,"Similar advances have been made in machine translation Frederking and Nirenburg 1994 , speech recognition Fiscus 1997 and named entity recognition Borthwick et al. 1998 .",Frederking and Nirenburg 1994,87,PMot,PMot,UNK,4,TRUE,55,#b5,bibr,,"(Frederking and Nirenburg, 1994)",2,-2,7,N,"Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998;Brill and Wu, 1998).",2,-1,8,N,In both cases the investigators were able to achieve significant improvements over the previous best tagging results.,2,0,9,Y,"Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).",2,1,10,N,"The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997).",TRUE,"4,5,6",TRUE,9,Introduction,1,Brill and Wu 1998,151,PMot,PMot,UNK,3,TRUE,132,#b1,bibr,"Brill and Wu, 1998)",,,,,,,,,,,,motivate approach,2,2,11,N,"These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).",2,3,12,N,We used these three parsers to explore parser combination techniques.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fiscus 1997,122,PMot,PMot,UNK,5,TRUE,108,#b4,bibr,"(Fiscus, 1997)",Borthwick et al. 1998,176,PMot,PMot,UNK,6,TRUE,152,#b0,bibr,"(Borthwick et al., 1998)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Several MB modules have been developed in previous work , such as : a POS tagger Daelemans et al. 1996 , a chunker Veenstra 1998 , Sang and Veenstra 1999 and a grammatical relation ( GR ) assigner Buchholz 1998 .",Daelemans et al. 1996,103,PMot,PMot,UNK,0,TRUE,79,#b7,bibr,,"(Daelemans et al., 1996)",0,-2,8,N,"For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.",0,-1,9,Y,In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence.,0,0,10,Y,"Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998;Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998).",0,1,11,Y,The questions we will answer in this paper are:,0,2,12,Y,Can we reuse these modules in a cascade of classifiers?,TRUE,"0,1,2,3",TRUE,10,Introduction,1,W99-0629,0,3,13,N,What is the effect of cascading?,,,,,"Collins 1997 , Ratnaparkhi 1997 use cascaded processing for full parsing with good results .",Collins 1997,15,PMot,PMot,UNK,6,TRUE,0,#b6,bibr,,"(Collins, 1997;",3,-2,18,N,"Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions.",3,-1,19,N,Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree.,3,0,20,Y,"(Collins, 1997;Ratnaparkhi, 1997) use cascaded processing for full parsing with good results.",3,1,21,N,Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification.,TRUE,"6,7",TRUE,20,Introduction,1,Veenstra 1998,131,PMot,PMot,UNK,1,TRUE,115,#b18,bibr,"(Veenstra, 1998;",Sang and Veenstra 1999,165,PMot,PMot,UNK,2,TRUE,131,#b17,bibr,"Tjong Kim Sang and Veenstra, 1999)",motivation for research problem,3,2,22,N,"However, their subject and object finders are independent of their chunker (i.e. not cascaded).",3,3,23,N,Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade.,"In recent work Daelemans et al. 1999b have shown that for typical natural language processing tasks , this approach is at an advantage because it also `` remembers '' exceptional , low-frequency cases which are useful to extrapolate from .",Daelemans et al. 1999b,39,PMot,PMot,UNK,9,TRUE,15,#b10,bibr,Daelemans et al. (1999b),5,-1,34,Y,Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory.,5,0,35,Y,"In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also ""remembers"" exceptional, low-frequency cases which are useful to extrapolate from.",5,1,36,Y,"Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997).",5,2,37,Y,We have used the following MBL algorithms1: IB1 :,5,3,38,N,A variant of the k-nearest neighbor (k-NN) algorithm.,TRUE,9,TRUE,35,Memory-Based Learning,2,"Moreover , automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources , as it embodies a smoothing-by-similarity method when data is sparse Zavrel and Daelemans 1997 .",Zavrel and Daelemans 1997,278,PMot,PMot,UNK,10,TRUE,250,#b19,bibr,"(Zavrel and Daelemans, 1997)",,,,,,,,,,,,,,,,,,,,,,,6,-2,34,N,Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory.,6,-1,35,N,"In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also ""remembers"" exceptional, low-frequency cases which are useful to extrapolate from.",6,0,36,Y,"Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997).",6,1,37,Y,We have used the following MBL algorithms1: IB1 :,6,2,38,N,A variant of the k-nearest neighbor (k-NN) algorithm.,6,3,39,N,The distance between a test item and each memory item is defined as the number of features for which they have a different value (overlap metric).,TRUE,10,TRUE,36,Memory-Based Learning,2,Ratnaparkhi 1997,33,PMot,PMot,UNK,7,TRUE,15,#b15,bibr,"Ratnaparkhi, 1997)",,,,,,,,,,,,,,,,,,,,,,,,,,TRUE,,,,Buchholz 1998,223,PMot,PMot,UNK,3,TRUE,207,#b4,bibr,"(Buchholz, 1998)",,,TRUE,
